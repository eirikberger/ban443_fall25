[["index.html", "A Technical Companion for BAN443 A Practical Guide to Python, APIs, and LLM Applications Chapter 1 Welcome to BAN443 1.1 Course Schedule 1.2 Compulsory Activity 1.3 Assessment 1.4 Course Overview 1.5 Getting Started 1.6 Contact and Support", " A Technical Companion for BAN443 A Practical Guide to Python, APIs, and LLM Applications Eirik Berger Abel 2025-09-12 Chapter 1 Welcome to BAN443 Welcome to this technical companion for the course BAN443 at the Norwegian School of Economics. Please note that this material covers only the technical portion of the overall curriculum. For a complete overview of the course, including non-technical content, visit the official course webpage: Transforming Business with AI: The Power of Large Language Models. Do not hesitate to contact me if you have any questions. This course is about AI and Large Language Models (LLMs) and how they can be used to transform business, not about coding or data analysis in itself. We will go through some basic programming using Python and APIs to get you started. Then, I will provide you with a lot of coding examples that we go through in class. If you feel comfortable with the material, you can try your own solutions and find other uses cases for the LLMs. If not, the code makes it possible to do a large range of analysis with just replacing some input, data and prompts. Feel free to use Chatgpt to help with any adjustments or importing and cleaning of data. 1.1 Course Schedule Week Date(s) Time Lecture Lecturer(s) Topic / Activity 34 Aug 19 14:15–16:00 Lecture 1 Ingar Haaland Introduction to the course and how AI transforms the business landscape 34 Aug 20 12:15–14:00 Lecture 2 Ingar Haaland Integrating AI into economic research 35 Aug 26 14:15–16:00 Lecture 3 Eirik Berger Abel Intro to Python and LLMs (Part 1) 35 Aug 27 12:15–14:00 Lecture 4 Eirik Berger Abel Intro to Python and LLMs (Part 2) 36 Sep 2 14:15–16:00 Lecture 5 Eirik Berger Abel Working with the API (Part 1) 36 Sep 3 12:15–14:00 Lecture 6 Eirik Berger Abel Exercise session 37 Sep 9 14:15–16:00 Lecture 7 Juan Carlos Lopez Calvet Guest lecture 37 Sep 10 12:15–14:00 Lecture 8 Eirik Berger Abel Working with the API (Part 2) 38 Sep 16 14:15–16:00 Lecture 9 Eirik Berger Abel Exercise session 38 Sep 17 12:15–14:00 Lecture 10 Eirik Berger Abel Exercise session 39 Sep 23 14:15–16:00 Lecture 11 Ingar Haaland Research application: Conducting qualitative interviews with AI 39 Sep 24 12:15–14:00 Lecture 12 Erling Risa Guest lecture 40 Sep 30 14:15–16:00 Lecture 13 Ingar Haaland Introduction to the term paper 41–42 – – – Students Group work: prepare project ideas (no lectures) 43 Oct 21 14:15–16:00 Lecture 14 Students, Ingar Haaland Student presentations: project ideas (session 1) 43 Oct 22 12:15–14:00 Lecture 15 Students, Ingar Haaland Student presentations: project ideas (session 2) 44–45 – – – Students Group work on term paper (no lectures) 46 Nov 11 14:15–16:00 Lecture 16 Students, Ingar Haaland Student presentations: final results (session 1) 46 Nov 12 12:15–14:00 Lecture 17 Students, Ingar Haaland Student presentations: final results (session 2) 47–50 – – – Students Group work: polish final paper (no lectures) – Dec 12 23:59 – – Final paper due (WISEflow submission) 1.2 Compulsory Activity Two group presentations in front of the class. All group members need to be active and present during both presentations. Group size is restricted to 3-4 students, but students can ask for an exemption if they want to work in smaller/larger groups. Students also need to hand in slides for each presentation. The slides need to be comprehensive. Participation in four mandatory online surveys. The online surveys will be administered during class. The surveys will, among other things, include multiple choice questions on concepts covered in class. The surveys will give a pass/fail grade. It is necessary to pass at least 3 out of the 4 online surveys to get a course approval. The course approval is only valid for one semester. 1.3 Assessment The assessment is based on the final paper, and will be written in the same groups as with the presentations. The paper is due four weeks after the final presentation (i.e., Friday December 12; to be uploaded on WISEflow by 12.00 this day) and must be written in English. The paper is graded on the standard scale (A–F). Each group member gets the same grade. 1.4 Course Overview This website covers the essential tools needed to work with AI and Large Language Models at a basic level, mostly through the use of APIs. You’ll learn: Python Programming: Essential Python skills for AI and data analysis LLM Fundamentals: Understanding how language models work under the hood API Integration: Working with commercial and open-source LLM APIs LangChain Framework: Building sophisticated AI applications Practical Applications: Real-world projects including data scraping, analysis, and automation 1.5 Getting Started To get the most out of this course: Set up a Python environment (we recommend Google Colab for beginners) Follow along with the code examples Complete the hands-on exercises Experiment with variations of the examples 1.6 Contact and Support For questions about this course, please contact the instructor at eirik.berger@gmail.com with ‘BAN443’ included in the subject line. "],["intro.html", "Chapter 2 Introduction to Python 2.1 Why Python for AI? 2.2 Setting Up Your Environment 2.3 Essential Python Concepts 2.4 Working with APIs 2.5 Want to learn more? 2.6 Next Steps", " Chapter 2 Introduction to Python This chapter introduces the essential Python programming concepts you’ll need to work with AI and Large Language Models through the use of APIs. While the focus of this part of the course is on understanding and using LLMs, having solid Python fundamentals will make everything else much easier. Of course, even as you learn these basics, there will still be plenty you don’t know yet—and that’s totally normal! When you get stuck or have questions, just do what every coder does: ask Google or ChatGPT. That’s a big part of how real programming works. If you are reallu stuck, do not hesitate to ask me for help. 2.1 Why Python for AI? Python is the most popular language for AI and data science because: Ease of use: Its simple, readable syntax makes it easy to learn and write code quickly. Automation: Python lets you automate repetitive tasks, making workflows more efficient. Vast ecosystem: There are thousands of libraries for AI, data analysis, web development, and more (e.g., numpy, pandas, scikit-learn, transformers, langchain). Community support: A huge global community means lots of tutorials, forums, and help when you get stuck. In short, Python is the “lingua franca” of AI. If you know Python, you can use almost any AI tool or library. 2.2 Setting Up Your Environment 2.2.1 Option 1: Google Colab (Recommended for Beginners) Google Colab provides a free, cloud-based Python environment that’s perfect for learning: Go to colab.research.google.com Sign in with your Google account Create a new notebook Start coding immediately! 2.2.2 Option 2: Local Installation If you prefer to work locally, you can use Python in several ways—such as Jupyter notebooks, standalone .py scripts, for example through VS Code. In these cases, you’ll need to first install Python, and then set up your own environment and install the necessary packages (like pandas, numpy, matplotlib, requests, transformers, and langchain). There are many guides online for installing Python and setting up your environment on your operating system. Search for instructions specific to your platform (Windows, macOS, or Linux) and your preferred workflow. Make sure to install the basic packages you’ll need for AI and data science work before proceeding. 2.3 Essential Python Concepts 2.3.1 Variables and Data Types Python uses dynamic typing, meaning you don’t need to declare variable types explicitly. The interpreter automatically determines the type based on the value you assign. # ===== NUMBERS ===== # Integers (whole numbers) age = 25 # Integer type year = 2024 # Another integer # Floating-point numbers (decimals) height = 5.9 # Float type pi = 3.14159 # Another float # Boolean values (True/False) is_student = True # Boolean type is_working = False # Another boolean # ===== STRINGS ===== # Text data - can use single or double quotes name = &quot;Alice&quot; # String with double quotes message = &#39;Hello, World!&#39; # String with single quotes multiline = &quot;&quot;&quot;This is a multi-line string&quot;&quot;&quot; # Multi-line string # ===== LISTS (Arrays) ===== # Ordered collections that can hold different data types fruits = [&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;] # List of strings numbers = [1, 2, 3, 4, 5] # List of integers mixed_list = [&quot;apple&quot;, 42, True, 3.14] # Mixed data types # ===== DICTIONARIES (Key-Value Pairs) ===== # Unordered collections that store data as key-value pairs person = { &quot;name&quot;: &quot;Alice&quot;, # Key: &quot;name&quot;, Value: &quot;Alice&quot; &quot;age&quot;: 25, # Key: &quot;age&quot;, Value: 25 &quot;city&quot;: &quot;New York&quot; # Key: &quot;city&quot;, Value: &quot;New York&quot; } # Accessing dictionary values print(person[&quot;name&quot;]) # Output: Alice print(person.get(&quot;age&quot;)) # Output: 25 (safer method) 2.3.2 Functions Functions are reusable blocks of code that perform specific tasks. They help organize your code, make it more readable, and allow you to avoid repeating the same code multiple times. 2.3.2.1 Basic Function Structure def function_name(parameters): &quot;&quot;&quot; Docstring: Description of what the function does &quot;&quot;&quot; # Function body - the code that runs return result # Optional: return a value # Example: Simple greeting function def greet(name): &quot;&quot;&quot; A function that creates a personalized greeting. Parameters: name (str): The person&#39;s name to greet Returns: str: A greeting message &quot;&quot;&quot; return f&quot;Hello, {name}!&quot; # Using the function message = greet(&quot;Alice&quot;) print(message) # Output: Hello, Alice! 2.3.2.2 Function Parameters and Arguments # Function with multiple parameters def calculate_area(length, width): &quot;&quot;&quot; Calculate the area of a rectangle. Parameters: length (float): Length of the rectangle width (float): Width of the rectangle Returns: float: Area of the rectangle &quot;&quot;&quot; area = length * width # Calculate area return area # Using the function with arguments room_area = calculate_area(10.5, 8.2) print(f&quot;Room area: {room_area} square feet&quot;) # Output: Room area: 86.1 square feet # Function with default parameters def greet_with_title(name, title=&quot;Mr./Ms.&quot;): &quot;&quot;&quot; Greet someone with an optional title. Parameters: name (str): Person&#39;s name title (str): Optional title (default: &quot;Mr./Ms.&quot;) Returns: str: Formal greeting &quot;&quot;&quot; return f&quot;Hello, {title} {name}!&quot; # Using with default parameter greeting1 = greet_with_title(&quot;Smith&quot;) # Uses default title print(greeting1) # Output: Hello, Mr./Ms. Smith! # Using with custom title greeting2 = greet_with_title(&quot;Johnson&quot;, &quot;Dr.&quot;) print(greeting2) # Output: Hello, Dr. Johnson! 2.3.2.3 Functions for Data Work def process_text_data(text): &quot;&quot;&quot; Process text data for AI analysis. Parameters: text (str): Input text to process Returns: dict: Processed text information &quot;&quot;&quot; # Count words word_count = len(text.split()) # Count characters char_count = len(text) # Check if text is empty is_empty = len(text.strip()) == 0 # Return results as a dictionary return { &quot;word_count&quot;: word_count, &quot;character_count&quot;: char_count, &quot;is_empty&quot;: is_empty, &quot;processed_text&quot;: text.strip().lower() } # Using the function sample_text = &quot;Hello, this is a sample text for processing!&quot; result = process_text_data(sample_text) print(result) # Output: {&#39;word_count&#39;: 9, &#39;character_count&#39;: 44, &#39;is_empty&#39;: False, # &#39;processed_text&#39;: &#39;hello, this is a sample text for processing!&#39;} 2.3.3 Installing and Importing Packages Python packages are collections of modules (code files) that provide additional functionality. For AI and data science work, you’ll need to install and import several important packages. 2.3.3.1 Installing Packages in Notebooks # Install packages using pip (Python package installer) !pip install pandas # Install multiple packages at once !pip install pandas openai langchain-openai langchain_community 2.3.3.2 Importing Packages Once installed, you import packages using the import statement (note that these imports as just example packages): # ===== BASIC IMPORTS ===== import pandas as pd # Import pandas with alias &#39;pd&#39; import numpy as np # Import numpy with alias &#39;np&#39; import matplotlib.pyplot as plt # Import matplotlib plotting module # ===== SELECTIVE IMPORTS ===== from datetime import datetime # Import only specific functions/classes from requests import get, post # Import multiple functions from sklearn.model_selection import train_test_split # Import from submodule 2.3.4 Working with Data You’ll frequently work with structured data using pandas: import pandas as pd # pandas: powerful library for data manipulation and analysis, especially for tabular data import numpy as np # numpy: fundamental package for numerical computing, provides support for arrays and mathematical functions # ===== CREATING DATA ===== # Create a simple dataset using a dictionary data = { &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charlie&#39;, &#39;Diana&#39;], &#39;age&#39;: [25, 30, 35, 28], &#39;city&#39;: [&#39;New York&#39;, &#39;London&#39;, &#39;Tokyo&#39;, &#39;Paris&#39;], &#39;salary&#39;: [50000, 60000, 70000, 55000] } # Convert dictionary to DataFrame (pandas table) df = pd.DataFrame(data) print(&quot;Original DataFrame:&quot;) print(df) print() # Empty line for readability # ===== BASIC DATA OPERATIONS ===== # View basic information about the data print(&quot;DataFrame Info:&quot;) print(df.info()) print() # Get statistical summary print(&quot;Statistical Summary:&quot;) print(df.describe()) print() # Access specific columns print(&quot;Names only:&quot;) print(df[&#39;name&#39;]) print() # Filter data print(&quot;People over 30:&quot;) over_30 = df[df[&#39;age&#39;] &gt; 30] # Boolean indexing print(over_30) print() # ===== ADDING NEW COLUMNS ===== # Calculate age in months df[&#39;age_months&#39;] = df[&#39;age&#39;] * 12 print(&quot;DataFrame with age in months:&quot;) print(df) # ===== APPLYING FUNCTIONS TO COLUMNS ===== # Suppose we want to calculate the length of each person&#39;s name and store it in a new column df[&#39;name_length&#39;] = df[&#39;name&#39;].apply(len) print(&quot;DataFrame with name length:&quot;) print(df) print() # If you want to create multiple new columns from a function, return a Series from your function def split_city_salary(row): # Example: split city name into first 2 letters and last 2 letters return pd.Series({ &#39;city_start&#39;: row[&#39;city&#39;][:2], &#39;city_end&#39;: row[&#39;city&#39;][-2:] }) df[[&#39;city_start&#39;, &#39;city_end&#39;]] = df.apply(split_city_salary, axis=1) print(&quot;DataFrame with city_start and city_end columns:&quot;) print(df) print() 2.4 Working with APIs 2.4.1 What is an API? An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other. Just like a website has a URL (web address) that you visit to access a specific page or resource, an API also has a URL (called an “endpoint”) that you use to access specific data or functionality. When you make a request to an API, you’re essentially “visiting” a special web address, but instead of seeing a webpage, you get data (often in JSON format) that your program can use. So, interacting with an API is a lot like visiting a URL, but for computers to talk to each other instead of for humans to read. 2.4.2 Example of using API to get GoT quotes APIs use different types of HTTP requests. Below is an example of how to use a GET request to get quotes from the Game of Thrones API. import requests import json # ===== GET REQUEST ===== # Used to retrieve data. def get_got_quotes(number_of_quotes=1): &quot;&quot;&quot; Get Game of Thrones quotes. Parameters: number_of_quotes (int): Number of quotes to get &quot;&quot;&quot; # Example API endpoint (this is a fake URL for demonstration) url = f&quot;https://api.gameofthronesquotes.xyz/v1/random/{number_of_quotes}&quot; response = requests.get(url) data = response.json() # Convert JSON response to Python dictionary return data get_got_quotes(1) # {&#39;sentence&#39;: &#39;A ruler who hides behind paid executioners soon forgets what death is.&#39;, # &#39;character&#39;: {&#39;name&#39;: &#39;Eddard &quot;Ned&quot; Stark&#39;, # &#39;slug&#39;: &#39;ned&#39;, # &#39;house&#39;: {&#39;name&#39;: &#39;House Stark of Winterfell&#39;, &#39;slug&#39;: &#39;stark&#39;}}} 2.4.3 Working with JSON Objects APIs often send and receive data in JSON (JavaScript Object Notation) format. JSON is a lightweight, human-readable way to represent data as key-value pairs, similar to Python dictionaries. It’s widely used. import json # ===== JSON STRING EXAMPLE ===== # This is what you receive from an API (as a string) json_string = &#39;&#39;&#39; { &quot;name&quot;: &quot;Alice Johnson&quot;, &quot;age&quot;: 28, &quot;is_student&quot;: false, &quot;courses&quot;: [&quot;Python&quot;, &quot;Data Science&quot;, &quot;AI&quot;], &quot;address&quot;: { &quot;street&quot;: &quot;123 Main St&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;zipcode&quot;: &quot;10001&quot; }, &quot;grades&quot;: { &quot;python&quot;: 95, &quot;data_science&quot;: 88, &quot;ai&quot;: 92 } } &#39;&#39;&#39; # ===== CONVERTING JSON TO PYTHON DICTIONARY ===== # Parse JSON string into Python dictionary data = json.loads(json_string) print(&quot;Type of data:&quot;, type(data)) # &lt;class &#39;dict&#39;&gt; print(&quot;Full data:&quot;) print(data) print() # ===== ACCESSING JSON DATA (SAME AS DICTIONARY ACCESS) ===== # Access top-level values print(&quot;Name:&quot;, data[&quot;name&quot;]) # Alice Johnson print(&quot;Age:&quot;, data[&quot;age&quot;]) # 28 print(&quot;Is student:&quot;, data[&quot;is_student&quot;]) # False print(&quot;Courses:&quot;, data[&quot;courses&quot;]) # [&#39;Python&#39;, &#39;Data Science&#39;, &#39;AI&#39;] print() # ===== ACCESSING NESTED DATA ===== # Access nested dictionary (address) print(&quot;Street:&quot;, data[&quot;address&quot;][&quot;street&quot;]) # 123 Main St print(&quot;City:&quot;, data[&quot;address&quot;][&quot;city&quot;]) # New York print(&quot;Zipcode:&quot;, data[&quot;address&quot;][&quot;zipcode&quot;]) # 10001 print() # ===== ACCESSING ARRAY/LIST DATA ===== # Access list elements print(&quot;First course:&quot;, data[&quot;courses&quot;][0]) # Python print(&quot;All courses:&quot;, data[&quot;courses&quot;]) # [&#39;Python&#39;, &#39;Data Science&#39;, &#39;AI&#39;] print(&quot;Number of courses:&quot;, len(data[&quot;courses&quot;])) # 3 print() # ===== ACCESSING NESTED DICTIONARY VALUES ===== # Access grades print(&quot;Python grade:&quot;, data[&quot;grades&quot;][&quot;python&quot;]) # 95 print(&quot;Data Science grade:&quot;, data[&quot;grades&quot;][&quot;data_science&quot;]) # 88 print(&quot;AI grade:&quot;, data[&quot;grades&quot;][&quot;ai&quot;]) # 92 print() # ===== SAFE ACCESS METHODS ===== # Using .get() method to avoid KeyError print(&quot;Phone (safe):&quot;, data.get(&quot;phone&quot;, &quot;Not provided&quot;)) # Not provided print(&quot;Name (safe):&quot;, data.get(&quot;name&quot;, &quot;Unknown&quot;)) # Alice Johnson # ===== ITERATING THROUGH JSON DATA ===== print(&quot;=== Iterating through courses ===&quot;) for i, course in enumerate(data[&quot;courses&quot;], 1): print(f&quot;{i}. {course}&quot;) print(&quot;\\n=== Iterating through grades ===&quot;) for subject, grade in data[&quot;grades&quot;].items(): print(f&quot;{subject.title()}: {grade}&quot;) print(&quot;\\n=== Iterating through address ===&quot;) for key, value in data[&quot;address&quot;].items(): print(f&quot;{key.title()}: {value}&quot;) 2.4.3.1 Converting Python Dictionary to JSON # ===== CONVERTING PYTHON DICTIONARY TO JSON ===== # Create a Python dictionary student_data = { &quot;name&quot;: &quot;Bob Smith&quot;, &quot;age&quot;: 25, &quot;is_student&quot;: True, &quot;courses&quot;: [&quot;Machine Learning&quot;, &quot;Statistics&quot;, &quot;Python&quot;], &quot;contact&quot;: { &quot;email&quot;: &quot;bob@example.com&quot;, &quot;phone&quot;: &quot;555-1234&quot; }, &quot;scores&quot;: [85, 92, 78, 96] } # Convert dictionary to JSON string json_output = json.dumps(student_data, indent=2) # indent=2 for pretty printing print(&quot;JSON String:&quot;) print(json_output) print() # ===== WORKING WITH API RESPONSES ===== def process_api_response(): &quot;&quot;&quot; Simulate processing a real API response. &quot;&quot;&quot; # Simulate API response (this would come from requests.get().json()) api_response = { &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: [ { &quot;id&quot;: 1, &quot;title&quot;: &quot;Introduction to AI&quot;, &quot;author&quot;: &quot;Dr. Smith&quot;, &quot;published&quot;: &quot;2024-01-15&quot;, &quot;tags&quot;: [&quot;AI&quot;, &quot;Machine Learning&quot;, &quot;Python&quot;] }, { &quot;id&quot;: 2, &quot;title&quot;: &quot;Data Science Fundamentals&quot;, &quot;author&quot;: &quot;Dr. Johnson&quot;, &quot;published&quot;: &quot;2024-02-01&quot;, &quot;tags&quot;: [&quot;Data Science&quot;, &quot;Statistics&quot;, &quot;R&quot;] } ], &quot;total&quot;: 2 } # Process the response print(&quot;API Response Status:&quot;, api_response[&quot;status&quot;]) print(&quot;Total items:&quot;, api_response[&quot;total&quot;]) print() # Process each item in the data array for item in api_response[&quot;data&quot;]: print(f&quot;Title: {item[&#39;title&#39;]}&quot;) print(f&quot;Author: {item[&#39;author&#39;]}&quot;) print(f&quot;Published: {item[&#39;published&#39;]}&quot;) print(f&quot;Tags: {&#39;, &#39;.join(item[&#39;tags&#39;])}&quot;) print(&quot;-&quot; * 40) # Run the example process_api_response() 2.4.4 API Authentication Many APIs require authentication to prevent abuse and track usage. Here are two examples of how to do this with the requests package. However, note that when we use LLMs we will use special packages that will handle the API calls for us. All we need to do it provide the correct function with the API key. This is just to show what is happening behind the scenes. Important: The import statement loads the userdata module from Google Colab, which provides secure access to stored secrets and API keys in the Colab environment. It’s used to retrieve sensitive information like API keys without hardcoding them directly in the code. See the image below for how to add the API key to the Colab environment. Image # Example of API calls with different authentication methods. # NOTE THAT THIS CODE WILL NOT RUN, AS ITS JUST FOR ILLUSTRATION PURPOSES. # ===== API KEY FROM COLAB SECRETS ===== def api_key_auth_colab(): &quot;&quot;&quot; Using API key stored in Google Colab secrets (colab&#39;s data/secrets package). This is the recommended way to securely store secrets in Colab. &quot;&quot;&quot; api_key = userdata.get(&#39;API_KEY&#39;) headers = { &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;, &quot;Content-Type&quot;: &quot;application/json&quot; } url = &quot;https://api.example.com/data&quot; response = requests.get(url, headers=headers) return response def api_key_in_url_colab(): &quot;&quot;&quot; Using API key as URL parameter, retrieved from Colab secrets. &quot;&quot;&quot; api_key = userdata.get(&#39;API_KEY&#39;) url = f&quot;https://api.example.com/data?api_key={api_key}&quot; response = requests.get(url) return response # ===== NOTE: For local development, use environment variables or .env files to store your API keys securely. # If you&#39;re not sure how to do this, search Google or ask ChatGPT for &quot;how to use environment variables in Python&quot; or &quot;how to use a .env file in Python&quot;. ===== 2.4.5 Google Drive Integration When working with Google Colab, you often need to access files stored in your Google Drive. This is particularly useful for loading datasets, saving outputs, or accessing pre-trained models. Here’s how to mount your Google Drive: from google.colab import drive drive.mount(&#39;/content/drive&#39;) This will mount your Google Drive at /content/drive. You can now access your files using the path /content/drive/MyDrive/. 2.5 Want to learn more? There are many great resources online for learning more about Python. Here is a quick video introducting the very basics of Python, but you can easily find many more on youtube that goes in much more depth. 2.6 Next Steps Now that you have the Python basics, you’re ready to dive into the fascinating world of Large Language Models. In the next chapter, we’ll explore how these models work and get hands-on experience with GPT-2. Remember: Don’t worry if you’re not a Python expert yet (or ever). The most important thing is to start experimenting and learning through practice. Each class will build your skills progressively. "],["llm-fundamentals.html", "Chapter 3 Understanding Large Language Models 3.1 What Are Large Language Models? 3.2 Neural Networks: The Foundation 3.3 From Neural Networks to Language Models 3.4 How Language Models Work 3.5 Want to learn more? 3.6 Next Steps", " Chapter 3 Understanding Large Language Models In this chapter, we’ll explore the fundamentals of Large Language Models (LLMs) and how they work. This knowledge will help you use LLMs more effectively and understand their capabilities and limitations. 3.1 What Are Large Language Models? Large Language Models are AI systems trained on massive amounts of text data to understand and generate human-like language. At their most basic level, they are sophisticated autocomplete systems that predict the next word (or token) in a sequence. Data: Trained on massive text corpora from the internet Architecture: Based on transformer neural networks Core Function: Essentially does next token prediction (autocomplete) 3.2 Neural Networks: The Foundation Before diving into LLMs, let’s understand the basic building blocks. LLMs are built on neural networks - computing systems inspired by how our brains work. 3.2.1 What is a Neural Network? The components of a neural network are: Neurons (circles): Each circle represents a simple computing unit that can receive information, process it, and send it forward Connections (lines): The lines show how information flows from one neuron to another, and each line has a weight (like a β coefficient in regression) Layers: Neurons are organized in layers - input, hidden, and output The thickness of the connections in the diagram represents the strength of the weights - thicker lines mean stronger influence, just like larger β coefficients in regression have more impact on the prediction. Deep neural network Image source: IBM - Neural Networks Explained 3.2.2 How Information Flows Input Layer: Receives the raw data (like tokenized text) Hidden Layers: Process and transform the information through multiple steps Output Layer: Produces the final result (like the next token prediction) The “deep” in deep neural networks refers to having multiple hidden layers, allowing the network to learn increasingly complex patterns. 3.2.3 Neural Networks vs. Traditional Regression Think of a neural network almost like a multi-stage OLS regression, but much more sophisticated: 3.2.3.1 OLS Regression (Simple) In a basic regression, you predict Y using: Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ You find the best weights (β’s) that minimize prediction errors. Example: Predicting house price = β₀ + β₁(square feet) + β₂(bedrooms) + β₃(age) - If β₁ = 100, each extra square foot adds $100 to the price - If β₃ = -500, each year of age reduces price by $500 3.2.3.2 Neural Networks (Multi-Stage) A neural network does the same thing, but in multiple stages: Stage 1: Hidden₁ = f(β₁₁X₁ + β₁₂X₂ + ... + β₁ₙXₙ) where f is a simple function like ReLU or Sigmoid. Stage 2: Hidden₂ = f(β₂₁Hidden₁ + β₂₂Hidden₁ + ... + β₂ₘHidden₁) (again, using a simple activation function such as ReLU or Sigmoid) Stage 3: Output = f(β₃₁Hidden₂ + β₃₂Hidden₂ + ... + β₃ₖHidden₂) (the output layer often uses Softmax for classification tasks) Each connection between neurons has a weight (a β coefficient, just like in regression), and each neuron receives inputs (the X’s) from the previous layer. The output of a neuron is calculated as a weighted sum of its inputs (e.g., β₁₁X₁ + β₁₂X₂ + … + β₁ₙXₙ), passed through an activation function (like ReLU or Sigmoid). A big difference is that in OLS regression, you can solve for the best β weights analytically in one step, while in a neural network, the model has to “train”—gradually adjusting the weights over many iterations to get better and better at minimizing prediction errors. 3.2.3.3 The Key Difference OLS: One stage, linear relationship Neural Networks: Multiple stages, with non-linear functions (f) between stages Same Goal: Find the best weights to minimize prediction errors 3.2.4 How Neural Networks Learn Neural networks use supervised learning - just like OLS regression! During training: - Input: Text sequences (like “The cat sat on the”) - Target: The actual next word (like “mat”) - Goal: Adjust weights so the network predicts “mat” when given “The cat sat on the” The network learns by comparing its predictions to the correct answers and adjusting weights to reduce errors. (Here, we’re skipping over some really important details—like the process of backpropagation and stochastic gradient descent—which is how the network actually figures out how to adjust those weights. But for now, the key idea is that the network keeps tweaking its weights to get better at making predictions.) A great way to build intuition is to watch this short video: 3.3 From Neural Networks to Language Models Now that we understand how neural networks work, let’s see how they become language models. The key insight is that instead of predicting a single Y value, language models use neural networks to predict the next word in a sequence. Large language models also use a special architecture called a transformer to do this efficiently and at scale. Transformers are a big reason why modern LLMs are so powerful, but we won’t go into the details of transformers here. 3.3.1 The Language Modeling Task In language modeling, the neural network: 1. Takes a sequence of words as input 2. Processes them through multiple hidden layers 3. Outputs probabilities for what the next word should be 4. The “magic” is that with enough stages and the right weights, the network can learn incredibly complex patterns in language 3.4 How Language Models Work Now that we understand the foundation, let’s see how neural networks become language models in practice. 3.4.1 Tokenization: From Text to Numbers Before LLMs can process text, it must be converted into numbers. This process is called tokenization. 3.4.1.1 How Tokenization Works from transformers import GPT2Tokenizer # Load the GPT-2 tokenizer tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;) # Tokenize a word inputs = tokenizer(&quot;inequality&quot;, return_tensors=&quot;pt&quot;) print(&quot;Token IDs:&quot;, inputs.input_ids) print(&quot;Token IDs as list:&quot;, inputs.input_ids[0].tolist()) 3.4.1.2 Understanding Token IDs Let’s see what those numbers actually represent: # Convert IDs back to tokens (strings) ids = inputs.input_ids[0].tolist() tokens = [tokenizer.decode([i]) for i in ids] print(&quot;IDs:&quot;, ids) print(&quot;Tokens:&quot;, tokens) You’ll notice that “inequality” becomes [500, 13237] when using the GPT-2 tokenizer. This is specific to GPT-2—other models may use different tokenizers and produce different token IDs. This means: - The word is split into subword tokens - Each token gets a unique ID number - The model works with these numbers, not the original text 3.4.1.3 Why Subword Tokenization? Vocabulary Size: Instead of having a token for every possible word, models use subwords Unknown Words: Can handle new words by breaking them into known subwords 3.4.2 Training Large Language Models Training LLMs involves two main stages: a large-scale pre-training phase, where the model learns general language patterns from massive text datasets, followed (optionally) by a fine-tuning stage, where the model is adapted for specific tasks or aligned with human preferences. 3.4.2.1 1. Pre-training Phase Data: Massive text corpora (books, articles, websites) Task: Predict the next token in a sequence Duration: Weeks or months on powerful hardware Cost: Millions of dollars in compute resources During pre-training, the model is shown huge amounts of text from the web, books, and articles. For each training example, a small part of the text is hidden (masked), and the model is asked to predict the missing word or token based on the context before it. Example: Suppose the original sentence is: “Artificial intelligence is transforming the world.” The model might see: “Artificial intelligence is transforming the ____.” The model’s task is to predict the missing word (“world”) using the words before it. This process is repeated billions of times with different pieces of text and different masked tokens. Over time, the model learns the patterns, grammar, facts, and even some reasoning abilities from the data. This “fill-in-the-blank” task is what enables LLMs to generate coherent and relevant text when given a prompt. 3.4.2.2 2. Fine-tuning and Alignment (Optional) Data: Smaller, carefully chosen datasets focused on a domain or style Purpose: Adapt a general-purpose model to perform specific tasks (like customer support, coding help, or medical Q&amp;A) or to align it with human preferences (being polite, safe, and following instructions). How it works: The base model—already trained to predict the next token—is further trained or adjusted using examples such as Q&amp;A pairs, conversations, or code snippets. In addition to traditional fine-tuning, modern approaches often use reinforcement learning from human feedback (RLHF) to make the model’s behavior better match what users want. Examples: Teaching the model to follow instructions (e.g., “Summarize this article”) Improving accuracy for domain-specific tasks (e.g., legal or medical assistance) Shaping interaction style (e.g., friendly conversation, professional tone, storytelling) This stage is what turns a raw “autocomplete engine” into a useful assistant—capable not just of predicting text, but of responding helpfully, safely, and in ways that align with human expectations. 3.4.3 Next Token Prediction: The Core of LLMs At this point, the large language model has mastered the fundamental skill of predicting what comes next in a sequence of text. Given any input text, it can generate probabilities for thousands of possible next tokens, essentially learning the patterns and relationships that govern human language. 3.4.3.1 Hands-On Example Let’s see this in action with GPT-2: from transformers import GPT2TokenizerFast, GPT2LMHeadModel import torch # Load the model and tokenizer tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;) model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;) def get_next_tokens(text, num_tokens=10): &quot;&quot;&quot;Get the most likely next tokens for a given text.&quot;&quot;&quot; # Tokenize input inputs = tokenizer(text, return_tensors=&quot;pt&quot;) # Get model predictions with torch.no_grad(): outputs = model(**inputs) logits = outputs.logits[0, -1, :] # Last token&#39;s logits probabilities = torch.softmax(logits, dim=-1) # Get top predictions top_indices = torch.topk(probabilities, num_tokens).indices top_probs = torch.topk(probabilities, num_tokens).values results = [] for idx, prob in zip(top_indices, top_probs): token = tokenizer.decode([idx]) results.append((token, prob.item())) return results # Example usage text = &quot;The future of artificial intelligence is&quot; predictions = get_next_tokens(text) print(f&quot;Input: &#39;{text}&#39;&quot;) print(&quot;\\nTop 10 most likely next tokens:&quot;) for token, probability in predictions: print(f&quot; &#39;{token}&#39;: {probability:.3f}&quot;) 3.4.3.2 Sampling from the Probability Distribution When the model outputs probabilities for the next token, it doesn’t always pick the most likely one. Instead, it samples from the probability distribution. This is crucial for creating natural, varied text. Example: If the model predicts these probabilities for the next word after “The weather is”: - “sunny” (40%) - “rainy” (30%) - “cloudy” (20%) - “cold” (10%) The model could: - Always pick “sunny” (most likely) → boring, repetitive text - Sample randomly based on probabilities → natural, varied text Sampling means the model randomly selects a word, but with higher probability words being chosen more often. This creates the natural variation we see in human-like text generation. 3.4.3.3 Text Generation: Iterative Process To generate longer text, LLMs repeat this process: Start with initial text Predict probability distribution for next token Sample from the distribution (or pick the most likely) Add selected token to text Use new text to predict next token Repeat until desired length This is why words appear one after another, iteratively, in chatgpt.com. 3.5 Want to learn more? 3.6 Next Steps Now that you understand some of how LLMs work, you’re ready to start using them through APIs. In the next chapter, we’ll learn how to interact with commercial LLM services and build your first AI-powered applications. The key takeaway is that LLMs are powerful tools, but they’re not magic. Understanding their limitations and how they work will make you a much more effective user of these technologies. "],["working-with-llms-through-apis.html", "Chapter 4 Working with LLMs through APIs 4.1 Using the OpenAI Packages 4.2 Using LangChain 4.3 Using LangSmith to track your LLM usage 4.4 Using LangGraph to build more advanced LLM applications", " Chapter 4 Working with LLMs through APIs This chapter provides a guide to using Large Language Models (LLMs) through APIs, covering basic interactions with the LLM, creating and using prompt templates, and structured data extraction. We will show all this using both the openai package and langchain. Note: Using LLMs through APIs mean that somebody else is doing the hard work for you, we are essentially renting the model from them and paying for the usage. The alternative would be to run an open source model ourselfs, using our own hardware (like we did in the previous chapter with GPT-2). Why use both? What’s the difference? The openai package is the official Python client for directly interacting with OpenAI (and Azure OpenAI) models. It’s simple and great for basic LLM calls, such as sending a prompt and receiving a response. In contrast, langchain is a higher-level framework that sits on top of LLM APIs (including OpenAI) and enables more advanced workflows—such as chaining multiple LLM calls, integrating external tools, building agents, and managing complex prompts. A key advantage of langchain is that it can work with many different LLM APIs (OpenAI, Azure, Anthropic, Google, etc.) with minimal code changes. This makes it much easier to switch between providers or support multiple models in your application. 4.1 Using the OpenAI Packages The openai package is the official Python client for interacting with OpenAI and Azure OpenAI services. It provides a simple, intuitive interface for LLM interactions. 4.1.1 Installation First, install the required packages: # Install the OpenAI package !pip install openai 4.1.2 Essential Package Imports The two imports below serve different purposes: - from openai import OpenAI imports the standard OpenAI client for accessing OpenAI’s public API. - from openai import AzureOpenAI imports the client specifically designed for interacting with Azure OpenAI endpoint. Use OpenAI for OpenAI’s own API, and AzureOpenAI when working with Azure-hosted OpenAI models. Note that when using Azure OpenAI, the “model” parameter is just the deployment name (e.g., “Group01” in our case). 4.1.3 API Configuration For Azure OpenAI, we need to use the AzureOpenAI client. # Basic Azure OpenAI setup (using openai package) from openai import AzureOpenAI from google.colab import userdata client = AzureOpenAI( api_key=userdata.get(&#39;AZURE_OPENAI_API_KEY&#39;), api_version=&quot;2025-01-01-preview&quot;, base_url=&quot;https://gpt-ban443-1.openai.azure.com/openai/deployments/Group01/&quot; ) For OpenAI’s own API, we use the OpenAI client. # Standard OpenAI setup from openai import OpenAI from google.colab import userdata client = OpenAI( api_key=userdata.get(&#39;OPENAI_API_KEY&#39;) ) Once the client is set up (as shown above), you can use it throughout your code to make API calls. You do not need to re-initialize the client for each request—just use the same client object. This keeps your code clean and efficient. 4.1.4 Basic Chat Completion The most common way to interact with LLMs is through chat completions: # Simple chat completion response = client.chat.completions.create( model=&quot;Group01&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain machine learning in simple terms.&quot;} ], temperature=0.7, max_tokens=500 ) print(response.choices[0].message.content) 4.1.5 Message Object Structure Understanding the message object is crucial for effective LLM interactions: # Message roles and their purposes messages = [ { &quot;role&quot;: &quot;system&quot;, # Sets the behavior and context &quot;content&quot;: &quot;You are an expert data scientist.&quot; }, { &quot;role&quot;: &quot;user&quot;, # User&#39;s input/question &quot;content&quot;: &quot;How do I implement a neural network?&quot; }, { &quot;role&quot;: &quot;assistant&quot;, # Model&#39;s previous response &quot;content&quot;: &quot;Neural networks consist of layers of interconnected nodes...&quot; }, { &quot;role&quot;: &quot;user&quot;, # Follow-up question &quot;content&quot;: &quot;Can you provide a code example?&quot; } ] You can insert this message object back into the chat completion function to get a response from the LLM. # Simple chat completion response = client.chat.completions.create( model=&quot;Group01&quot;, messages=messages, temperature=0.7, max_tokens=500 ) print(response.choices[0].message.content) 4.1.6 Structured Data Extraction You can extract structured data from text using Pydantic models and the responses.parse() method. However, note that we are now switching to using client.responses instead of client.chat.completions. This uses a newer API form OpenAI that doesnt work with the ‘old’ LLM model that you used through Azure. We therefore switch to using the gpt-5-mini model. # Install required packages !pip install pydantic from openai import AzureOpenAI from google.colab import userdata client = AzureOpenAI( api_version= &quot;2025-03-01-preview&quot;, azure_endpoint=&quot;https://ban443-1.openai.azure.com/&quot;, api_key=userdata.get(&quot;new_azure_gpt5&quot;), ) from pydantic import BaseModel # Define the structure you want to extract class PersonInfo(BaseModel): name: str age: int occupation: str location: str # Extract structured data from text completion = client.responses.parse( model=&quot;Group01&quot;, input=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract person information from the text.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;John Smith is 35 years old, works as a data scientist, and lives in Oslo.&quot;} ], text_format=PersonInfo, ) # Get the structured result person = completion.output_parsed print(person.model_dump_json()) # Output: {&quot;name&quot;: &quot;John Smith&quot;, &quot;age&quot;: 35, &quot;occupation&quot;: &quot;data scientist&quot;, &quot;location&quot;: &quot;Oslo&quot;}``` 4.1.7 Creating Reusable Functions You can create functions to make your LLM interactions more organized and reusable. Here’s an example that combines several concepts: def ask(role: str, domain: str, question: str): &quot;&quot;&quot; Ask a question to an AI assistant with a specific role and domain expertise. Args: role: The role of the assistant (e.g., &quot;data scientist&quot;, &quot;teacher&quot;) domain: The domain of expertise (e.g., &quot;machine learning&quot;, &quot;statistics&quot;) question: The question to ask Returns: The AI&#39;s response as a string &quot;&quot;&quot; # Create messages using f-strings for dynamic content msgs = [ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: f&quot;You are a {role} with expertise in {domain}.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}, ] # Use the responses API for cleaner output resp = client.responses.create( model=&quot;Group01&quot;, input=msgs, ) return resp.output_text # Example usage answer = ask(&quot;data scientist&quot;, &quot;machine learning&quot;, &quot;How do I prevent overfitting?&quot;) print(answer) Key concepts explained: f-strings: The f\"...\" syntax allows you to insert variables directly into strings. For example, f\"You are a {role}\" becomes \"You are a data scientist\" when role = \"data scientist\". Function parameters: The function takes three parameters (role, domain, question) that can be different each time you call it. Docstrings: The triple-quoted string at the top of the function explains what it does and what parameters it expects. This is optional, but it’s a good practice to always include it. Return values: The function returns the AI’s response, which you can store in a variable or use directly. Reusability: You can now ask different questions to different types of experts without rewriting the code: # Ask a statistics expert stats_answer = ask(&quot;statistician&quot;, &quot;statistics&quot;, &quot;What is a p-value?&quot;) # Ask a business expert business_answer = ask(&quot;business analyst&quot;, &quot;finance&quot;, &quot;How do I calculate ROI?&quot;) # Ask a programming expert code_answer = ask(&quot;software engineer&quot;, &quot;Python&quot;, &quot;How do I handle exceptions?&quot;) 4.2 Using LangChain LangChain is a framework for developing applications powered by language models. It provides abstractions and tools for building complex LLM applications. At the most basic level its almost identical to the openai package, but it has much more functionality. 4.2.1 Installation First, install the required LangChain packages: # Install LangChain packages !pip install -U langchain-openai langchain_community 4.2.2 API Configuration For Azure OpenAI through LangChain, we use the AzureChatOpenAI class: # LangChain Azure OpenAI setup from langchain_openai import AzureChatOpenAI from google.colab import userdata client = AzureChatOpenAI( api_key=userdata.get(&quot;AZURE_OPENAI_API_KEY&quot;), azure_endpoint=&quot;https://gpt-ban443-1.openai.azure.com/&quot;, api_version=&quot;2025-01-01-preview&quot;, azure_deployment=&quot;Group01&quot;, temperature=0, ) For OpenAI’s own API through LangChain, we use the ChatOpenAI class: # LangChain standard OpenAI setup from langchain_openai import ChatOpenAI from google.colab import userdata client = ChatOpenAI( api_key=userdata.get(&quot;OPENAI_API_KEY&quot;), model=&quot;gpt-4&quot;, temperature=0.7 ) Once the LLM is set up (as shown above), you can use it throughout your code to make API calls. You do not need to re-initialize the LLM for each request—just use the same client object. This keeps your code clean and efficient. 4.2.3 Basic Chat Completion and Message Structure The most common way to interact with LLMs through LangChain is using the invoke method. LangChain uses its own message classes for structured communication: # LangChain message types and their purposes from langchain_core.messages import SystemMessage, HumanMessage, AIMessage messages = [ SystemMessage(content=&quot;You are an expert data scientist.&quot;), # Sets behavior and context HumanMessage(content=&quot;How do I implement a neural network?&quot;), # User&#39;s input/question AIMessage(content=&quot;Neural networks consist of layers of interconnected nodes...&quot;), # Model&#39;s previous response HumanMessage(content=&quot;Can you provide a code example?&quot;) # Follow-up question ] response = client.invoke(messages) print(response.content) Alternative: Using tuples for simpler prompts LangChain also supports a simpler tuple format for basic use cases: # Simple tuple format (easier for basic prompts) messages = [ (&quot;system&quot;, &quot;You are a helpful assistant that translates English to French.&quot;), (&quot;human&quot;, &quot;I love programming.&quot;) ] response = client.invoke(messages) print(response.content) 4.2.4 Structured Data Extraction LangChain excels at structured data extraction using Pydantic models and the with_structured_output() method: # Install required packages !pip install pydantic from pydantic import BaseModel, Field from typing import Optional, List from langchain_core.prompts import ChatPromptTemplate # Define the structure you want to extract class Person(BaseModel): &quot;&quot;&quot;Information about a person from text.&quot;&quot;&quot; name: Optional[str] = Field(default=None, description=&quot;Full name of the person&quot;) age: Optional[int] = Field(default=None, description=&quot;Age of the person&quot;) occupation: Optional[str] = Field(default=None, description=&quot;Job or profession&quot;) location: Optional[str] = Field(default=None, description=&quot;Where they live&quot;) # Create a prompt template prompt = ChatPromptTemplate.from_messages([ (&quot;system&quot;, &quot;You are an expert at extracting person information from text. Extract all people mentioned.&quot;), (&quot;human&quot;, &quot;{text}&quot;) ]) # Create the structured extractor extractor = prompt | client.with_structured_output( schema=Person, include_raw=True, ) # Extract structured data response = extractor.invoke({ &quot;text&quot;: &quot;John Smith is 35 years old and works as a data scientist in Oslo.&quot; }) # Get the structured result people_info = response[&#39;parsed&#39;] print(people_info.model_dump_json()) # {&quot;name&quot;:&quot;John Smith&quot;,&quot;age&quot;:35,&quot;occupation&quot;:&quot;data scientist&quot;,&quot;location&quot;:&quot;Oslo&quot;} Note that if the input string contains multiple people, you simply need to add the following code and set the schema to PeopleInfo. As you can see, the PeopleInfo class is a list of Person objects. class PeopleInfo(BaseModel): &quot;&quot;&quot;Information about all people mentioned in the text.&quot;&quot;&quot; people: List[Person] 4.2.5 Prompt Templates LangChain’s strength lies in its prompt management: # Create a prompt template prompt = ChatPromptTemplate.from_messages([ (&quot;system&quot;, &quot;You are a {role} with expertise in {domain}.&quot;), (&quot;user&quot;, &quot;{question}&quot;) ]) chain = prompt | client response = chain.invoke({ &quot;role&quot;: &quot;data scientist&quot;, &quot;domain&quot;: &quot;machine learning&quot;, &quot;question&quot;: &quot;How do I prevent overfitting?&quot; }) print(response.content) 4.3 Using LangSmith to track your LLM usage A problem running many requests using APIs is that you lack visibility into what is happening. You don’t nessecarily know how many requests you are making, how long they take, or what the results are. This makes debugging and optimizing your code difficult. This is where LangSmith comes in. Luckely, the setup is very easy. You just need to visit https://smith.langchain.com/ and create an account. Then, create a new project. You can name it whatever you want, but make sure to copy the project ID/name and the API key. As always, you need to set the environment variables in Colab in the sidebar and retrive it using the userdata.get() function. Once you run this code, all LLM requests will be tracked in LangSmith (check that it works). import os from google.colab import userdata os.environ[&quot;LANGSMITH_TRACING&quot;] = &quot;true&quot; os.environ[&quot;LANGSMITH_ENDPOINT&quot;] = &quot;https://api.smith.langchain.com&quot; os.environ[&quot;LANGSMITH_API_KEY&quot;] = userdata.get(&#39;langsmith&#39;) os.environ[&quot;LANGSMITH_PROJECT&quot;] = &quot;pr-jaunty-eyeball-33&quot; 4.4 Using LangGraph to build more advanced LLM applications LangGraph is a framework for building more advanced LLM applications. It is built on top of LangChain and provides a more advanced way to build LLM applications. It is particularly useful for building agents, RAG systems, and complex multi-step reasoning applications. See their website for more information and examples (https://langchain-ai.github.io/langgraph/). # Install LangGraph !pip install langgraph 4.4.1 Building Chatbots with LangGraph LangGraph makes it easy to build conversational chatbots. We’ll demonstrate two types: a simple chatbot without memory and an advanced chatbot with memory and tool integration. 4.4.2 Simple Chatbot (Without Memory) This is a basic chatbot that responds to each message independently, without remembering previous conversations: from typing import Annotated from langchain_openai import AzureChatOpenAI from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from google.colab import userdata # Initialize the LLM client = AzureChatOpenAI( api_key=userdata.get(&quot;AZURE_OPENAI_API_KEY&quot;), azure_endpoint=&quot;https://gpt-ban443-1.openai.azure.com/&quot;, api_version=&quot;2025-01-01-preview&quot;, azure_deployment=&quot;Group01&quot;, temperature=0, ) # Define the state for our chatbot class State(TypedDict): messages: Annotated[list, add_messages] # Create the graph builder graph_builder = StateGraph(State) # Define the chatbot function def chatbot(state: State): &quot;&quot;&quot;Process user input and generate a response.&quot;&quot;&quot; return {&quot;messages&quot;: [client.invoke(state[&quot;messages&quot;])]} # Add the chatbot node to the graph graph_builder.add_node(&quot;chatbot&quot;, chatbot) # Define the flow: START -&gt; chatbot -&gt; END graph_builder.add_edge(START, &quot;chatbot&quot;) graph_builder.add_edge(&quot;chatbot&quot;, END) # Compile the graph graph = graph_builder.compile() # Function to interact with the chatbot def stream_graph_updates(user_input: str): &quot;&quot;&quot;Send user input to the chatbot and display the response.&quot;&quot;&quot; for event in graph.stream({&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]}): for value in event.values(): print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content) # Example usage print(&quot;Simple Chatbot (No Memory)&quot;) print(&quot;Type &#39;quit&#39;, &#39;exit&#39;, or &#39;q&#39; to stop&quot;) print(&quot;-&quot; * 40) while True: try: user_input = input(&quot;User: &quot;) if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]: print(&quot;Goodbye!&quot;) break stream_graph_updates(user_input) except: break 4.4.3 Advanced Chatbot (With Memory) This chatbot remembers previous conversations and maintains context throughout the conversation: from typing import Annotated from langchain_openai import AzureChatOpenAI from typing_extensions import TypedDict from langgraph.checkpoint.memory import InMemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from google.colab import userdata # Initialize the LLM client = AzureChatOpenAI( api_key=userdata.get(&quot;AZURE_OPENAI_API_KEY&quot;), azure_endpoint=&quot;https://gpt-ban443-1.openai.azure.com/&quot;, api_version=&quot;2025-01-01-preview&quot;, azure_deployment=&quot;Group01&quot;, temperature=0, ) # Define the state for our advanced chatbot class State(TypedDict): messages: Annotated[list, add_messages] # Create the graph builder graph_builder = StateGraph(State) # Define the chatbot function def chatbot(state: State): &quot;&quot;&quot;Process user input and generate a response with memory.&quot;&quot;&quot; return {&quot;messages&quot;: [client.invoke(state[&quot;messages&quot;])]} # Add the chatbot node to the graph graph_builder.add_node(&quot;chatbot&quot;, chatbot) # Define the flow: START -&gt; chatbot -&gt; END graph_builder.add_edge(START, &quot;chatbot&quot;) graph_builder.add_edge(&quot;chatbot&quot;, END) # Add memory using InMemorySaver memory = InMemorySaver() graph = graph_builder.compile(checkpointer=memory) # Configuration for maintaining conversation thread config = {&quot;configurable&quot;: {&quot;thread_id&quot;: &quot;1&quot;}} # Function to interact with the advanced chatbot def stream_graph_updates(user_input: str): &quot;&quot;&quot;Send user input to the chatbot and display the response.&quot;&quot;&quot; for event in graph.stream({&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]}, config): for value in event.values(): print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content) # Example usage print(&quot;Advanced Chatbot (With Memory)&quot;) print(&quot;Type &#39;quit&#39;, &#39;exit&#39;, or &#39;q&#39; to stop&quot;) print(&quot;-&quot; * 40) while True: try: user_input = input(&quot;User: &quot;) if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]: print(&quot;Goodbye!&quot;) break stream_graph_updates(user_input) except: break "],["applications.html", "Chapter 5 Applications 5.1 Data Extraction from the Norwegian National Library 5.2 Web Search and Tools Integration 5.3 Retrieval-Augmented Generation (RAG) Systems 5.4 Company Data Extraction from Brønnøysundregisteret", " Chapter 5 Applications This chapter demonstrates real-world applications of Large Language Models (LLMs) using the LangChain framework. We’ll explore various use cases ranging from data extraction from the Norwegian National Library to political analysis, web search integration, RAG systems, and automated company data extraction from Brønnøysundregisteret. All applications in this chapter use LangChain for their implementation, showcasing the framework’s capabilities for structured outputs, document processing, agent systems, and retrieval-augmented generation. Note that you might want to mount your Google Drive to Colab to save the results of your applications. from google.colab import drive drive.mount(&#39;/content/drive&#39;) This will mount your Google Drive at /content/drive. You can now access your files using the path /content/drive/MyDrive/. You can copy the path of a file by right clicking on it and selecting “Copy path” in the meny to the left after mounting the drive. 5.1 Data Extraction from the Norwegian National Library The Norwegian National Library provides access to digitized historical documents through their API. This section demonstrates how to extract and structure information from these documents using LangChain. We will use a historical address book from from Oslo as an example (https://www.nb.no/items/7e1566b1e568f37667019c24fe5cccd1). 5.1.1 Setting up LangChain with Azure OpenAI from langchain_openai import AzureChatOpenAI from langchain_core.prompts import ChatPromptTemplate from pydantic import BaseModel, Field from typing import Optional, List # Initialize LangChain with Azure OpenAI client = AzureChatOpenAI( api_key=userdata.get(&quot;AZURE_OPENAI_API_KEY&quot;), azure_endpoint=&quot;https://gpt-ban443-1.openai.azure.com/&quot;, api_version=&quot;2025-01-01-preview&quot;, azure_deployment=&quot;Group01&quot;, temperature=0, ) 5.1.2 Structured Data Extraction # Define Pydantic models for structured extraction class entity(BaseModel): &quot;&quot;&quot;Structure data about a person from noisy text.&quot;&quot;&quot; surname: Optional[str] = Field( default=None, description=&quot;The surname of a person. A dash indicates that the surname is same as last person (indicate this with &#39;-&#39;)&quot; ) firstname: Optional[str] = Field( default=None, description=&quot;The firstname of a person. Listed after a surname and a comma or a dash.&quot; ) occupation: Optional[str] = Field( default=None, description=&quot;The occupation of that person, not including firm names in parenthesis.&quot; ) firm: Optional[str] = Field( default=None, description=&quot;The firm that a person works at. It is included in parentheses (if at all).&quot; ) address: Optional[str] = Field( default=None, description=&quot;The place of residence for the person&quot; ) class entities(BaseModel): &quot;&quot;&quot;Identifying information about all people in a text.&quot;&quot;&quot; people: List[entity] # Create extraction prompt template prompt = ChatPromptTemplate.from_messages([ ( &quot;system&quot;, &quot;&quot;&quot;You are an expert at identifying information on individuals from address books. Only extract information on individuals and ignore what appears to be noise. Extract nothing if no important information can be found in the text. The content you see is from OCR scans of historical documents and will be noisy, so if there are obvious errors you can correct them in your response. If surname is indicated with a dash (&#39;-&#39;), then include this as surname, the rest of the name is in firstname. All names should start with upper case letters, while occupations start with lower case letters. Firm names in parenthesis should not be included in the occupation, but in the firm variable. Everyone should have a firstname. A typical observation in the book are like this: Berger, Eirik, forsker (SSB), Oscars gt. 999 &quot;&quot;&quot;, ), (&quot;human&quot;, &quot;{text}&quot;), ]) # Create structured extraction chain extractor = prompt | client.with_structured_output( schema=entities, include_raw=True, ) # Extract information from text response = extractor.invoke(&quot;My name is Eirik Abel, and I was born in 1993. I am a student. There is another person named Barne Ollson who is 34 years old and lives in Bergen&quot;) print(response[&#39;parsed&#39;].people[0].model_dump_json()) 5.1.3 Fetching Data from the Norwegian National Library !pip install wget !pip install PyMuPDF import wget import fitz from langchain_text_splitters import CharacterTextSplitter # Define the page range you want to extract from the National Library start_page = 500 end_page = 520 # Downloading a file from the Norwegian National Library url = f&quot;https://www.nb.no/services/downloader?urn=URN:NBN:no-nb_digitidsskrift_2019091381014_001&amp;resolutionlevel=4&amp;pg={start_page}-{end_page}&amp;text=true&amp;filename=2024-10-06T1216_NB_generated&quot; wget.download(url, &quot;address_book.pdf&quot;) # Extract text from PDF doc = fitz.open(&quot;address_book.pdf&quot;) all_text = &quot;&quot; for page_num in range(0, end_page - start_page): page = doc.load_page(page_num) all_text += page.get_text(&quot;text&quot;) # Split text into manageable chunks text_splitter = CharacterTextSplitter( separator=&quot;\\n&quot;, chunk_size=500, chunk_overlap=0, length_function=len, is_separator_regex=False, ) texts = text_splitter.create_documents([all_text]) print(f&quot;Created {len(texts)} text chunks from National Library data&quot;) 5.1.4 Batch Processing with Caching Setting up caching. This means that the LLM will remember the results of the previous calls, so that it doesn’t have to re-compute the same thing. # Cache setup. # https://python.langchain.com/docs/integrations/llm_caching/ from langchain.globals import set_llm_cache ## Using in memory cache: from langchain_community.cache import InMemoryCache set_llm_cache(InMemoryCache()) ## OR use SQLite cache: # from langchain_community.cache import SQLiteCache # set_llm_cache(SQLiteCache(database_path=&quot;.langchain.db&quot;)) # Use either memory cache or SQLite cache. Process 10 random chunks of text in parallel: import random random.seed(100) random_elements = random.sample(texts, 10) print(&quot;Processing documents with caching enabled...&quot;) extractions = extractor.batch( [{&quot;text&quot;: text.page_content} for text in random_elements], {&quot;max_concurrency&quot;: 5}, ) print(f&quot;Completed processing {len(extractions)} documents&quot;) 5.1.5 Converting JSON Results to DataFrame Lets convert results from the batch results to a dataframe. We use the result from the batch extraction above. Note that this is the kind of tasks that chatgpt can help you produce code for if you just give it the object you want to convert. # For batch results, process all extractions df_list = [] for index, result in enumerate(extractions): try: # Get the parsed JSON data json_data = result[&#39;parsed&#39;].model_dump_json() parsed_data = json.loads(json_data) # Convert to DataFrame df = pd.json_normalize(parsed_data[&#39;people&#39;]) df[&#39;file_index&#39;] = index df_list.append(df) except Exception as e: print(f&quot;Error processing document {index}: {e}&quot;) # Combine all results combined_df = pd.concat(df_list, ignore_index=True) print(f&quot;\\nCombined DataFrame shape: {combined_df.shape}&quot;) print(&quot;\\nFirst few rows:&quot;) print(combined_df.head()) Second, lets convert the response from a single element/answer to a dataframe. # Import import json import pandas as pd # Simple example: Convert a single extraction result to DataFrame sample_result = extractor.invoke(&quot;Berger, Eirik, forsker (SSB), Oscars gt. 999&quot;) # Extract the JSON data json_data = sample_result[&#39;parsed&#39;].model_dump_json() parsed_data = json.loads(json_data) # Convert to DataFrame df = pd.json_normalize(parsed_data[&#39;people&#39;]) print(&quot;Sample DataFrame:&quot;) print(df) 5.1.6 Analyzing Occupation Data import matplotlib.pyplot as plt # helps make the figure # Clean and analyze occupation data occupation_data = combined_df.dropna(subset=[&#39;occupation&#39;]) occupation_counts = occupation_data[&#39;occupation&#39;].value_counts().head(10) print(&quot;Top 10 Most Common Occupations:&quot;) print(occupation_counts) # Create visualization plt.figure(figsize=(12, 8)) occupation_counts.plot(kind=&#39;bar&#39;, color=&#39;skyblue&#39;, edgecolor=&#39;navy&#39;, alpha=0.7) plt.title(&#39;Top 10 Most Common Occupations from Norwegian National Library Data&#39;, fontsize=14, fontweight=&#39;bold&#39;) plt.xlabel(&#39;Occupation&#39;, fontsize=12) plt.ylabel(&#39;Number of People&#39;, fontsize=12) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.grid(axis=&#39;y&#39;, alpha=0.3) plt.tight_layout() plt.show() # Additional analysis: Geographic distribution if &#39;address&#39; in combined_df.columns: address_data = combined_df.dropna(subset=[&#39;address&#39;]) print(f&quot;\\nGeographic distribution (top 10):&quot;) print(address_data[&#39;address&#39;].value_counts().head(10)) 5.2 Web Search and Tools Integration LangChain provides powerful tools for integrating web search and other external APIs into LLM workflows. We first need to get an API key for Tavily from their website (https://www.tavily.com/). import os from google.colab import userdata !pip install langchain_tavily os.environ[&quot;TAVILY_API_KEY&quot;] = userdata.get(&#39;tavily&#39;) # This is where you include your API key! 5.2.1 Basic Web Search with Tavily from langchain_tavily import TavilySearch # Initialize search tool tool = TavilySearch(max_results=2) tools = [tool] # Perform search search_results = tool.invoke(&quot;What was the result of the football match between Norway and Moldova?&quot;) print(search_results) 5.2.2 Agent with Web Search Capabilities We will now use the langgraph package to work with more advanced agentic systems. LangGraph is particularly useful for building conversational agents, RAG systems, and complex multi-step reasoning applications. Most of the examples where we use LangGraph is taken from their website (https://langchain-ai.github.io/langgraph/). Start by installing it. !pip install langgraph Then, we can use the following code to build an agent with web search capabilities. from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition from typing import Annotated from typing_extensions import TypedDict class State(TypedDict): messages: Annotated[list, add_messages] # Create agent with search capabilities graph_builder = StateGraph(State) tool = TavilySearch(max_results=2) tools = [tool] llm_with_tools = client.bind_tools(tools) def chatbot(state: State): return {&quot;messages&quot;: [llm_with_tools.invoke(state[&quot;messages&quot;])]} graph_builder.add_node(&quot;chatbot&quot;, chatbot) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(&quot;tools&quot;, tool_node) graph_builder.add_conditional_edges( &quot;chatbot&quot;, tools_condition, ) graph_builder.add_edge(&quot;tools&quot;, &quot;chatbot&quot;) graph_builder.add_edge(START, &quot;chatbot&quot;) graph = graph_builder.compile() # Use the agent def stream_graph_updates(user_input: str): for event in graph.stream({&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]}): for value in event.values(): print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content) # Example usage stream_graph_updates(&quot;What&#39;s the latest news about NHH?&quot;) 5.3 Retrieval-Augmented Generation (RAG) Systems Next we want to create a RAG system. We often want to give our LLMs more knowledge and data to work with. This can be done by 1) Training the model with more data, 2) Including data directly in the prompt, or 3) Using a RAG system. As option 1 is often expensive and option 2 is often too limiting, option 3 has become popular. Note however that we need an embedding model to make this work, which we don’t have an API key for this class yet. You can create your own API key at https://platform.openai.com/ if you want to try it out, but note that it cost money depending on how much you use it (although not much). 5.3.1 Basic RAG Implementation from langchain_openai import OpenAIEmbeddings, ChatOpenAI from langchain_core.vectorstores import InMemoryVectorStore from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langgraph.graph import START, StateGraph, END from typing_extensions import List, TypedDict from langchain_core.documents import Document import bs4 from langchain import hub # Setup embeddings and vector store client = ChatOpenAI(api_key=userdata.get(&#39;new_openai&#39;), model=&quot;gpt-4o-mini&quot;) embeddings = OpenAIEmbeddings(api_key=userdata.get(&#39;new_openai&#39;)) vector_store = InMemoryVectorStore(embeddings) # Load and chunk documents loader = WebBaseLoader( web_paths=(&quot;https://nhhs.no/nhhs-fra-a-til-a/&quot;,&quot;https://nhhs.no/interessegrupper/&quot;, &quot;https://www.dn.no/&quot;), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(&quot;post-content&quot;, &quot;post-title&quot;, &quot;post-header&quot;) ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) all_splits = text_splitter.split_documents(docs) # Index chunks _ = vector_store.add_documents(documents=all_splits) # Define prompt for question-answering # Pull a pre-built RAG prompt template from LangChain Hub # This prompt is specifically designed for retrieval-augmented generation (RAG) tasks # It includes placeholders for question and context that will be filled in during execution prompt = hub.pull(&quot;rlm/rag-prompt&quot;) # Define RAG state class State(TypedDict): question: str context: List[Document] answer: str # Define RAG functions def retrieve(state: State): retrieved_docs = vector_store.similarity_search(state[&quot;question&quot;]) return {&quot;context&quot;: retrieved_docs} def generate(state: State): docs_content = &quot;\\n\\n&quot;.join(doc.page_content for doc in state[&quot;context&quot;]) messages = prompt.invoke({&quot;question&quot;: state[&quot;question&quot;], &quot;context&quot;: docs_content}) response = client.invoke(messages) return {&quot;answer&quot;: response.content} # Build RAG graph builder = StateGraph(State) builder.add_node(&quot;retrieve&quot;, retrieve) builder.add_node(&quot;generate&quot;, generate) builder.add_edge(START, &quot;retrieve&quot;) builder.add_edge(&quot;retrieve&quot;, &quot;generate&quot;) builder.add_edge(&quot;generate&quot;, END) graph = builder.compile() # Use RAG system response = graph.invoke({&quot;question&quot;: &quot;What&#39;s &#39;ASAP&#39;? Answer in english.&quot;}) print(response[&quot;answer&quot;]) 5.3.2 Persistent Vector Database The last example created a vector database in memory, which will dissapear once we end our notebook session. We can instead save the database to a file, so that it persists between sessions. !pip install -U langchain-chroma chromadb from langchain_community.vectorstores import Chroma # Choose a stable collection name + persist dir collection_name = &quot;nhhs_docs&quot; persist_dir = &quot;./chroma_db&quot; # --- Create / upsert (auto-persisted; no .persist() needed) --- vector_store = Chroma.from_documents( documents=all_splits, embedding=embeddings, collection_name=collection_name, persist_directory=persist_dir, ) # --- Load an existing DB later --- vector_store = Chroma( collection_name=collection_name, persist_directory=persist_dir, embedding_function=embeddings, ) # --- Query or use it in the RAG system --- results = vector_store.similarity_search(&quot;NHH student groups&quot;, k=3) for doc in results: print(f&quot;Content: {doc.page_content[:200]}...&quot;) print(f&quot;Source: {doc.metadata.get(&#39;source&#39;, &#39;Unknown&#39;)}&quot;) print(&quot;---&quot;) 5.4 Company Data Extraction from Brønnøysundregisteret This section demonstrates automated extraction of financial and employee data from company annual reports using LangChain’s structured output capabilities. This is a potentially very valuable application in a long range of sectors. This is quite complex compared that what we have previously looked at, but it’s not expected that you understand all of this code. Its mainly a showcase of what is possible with LLMs, and if you would want to do something similar, then you could use this as a starting point. I first get data from their open API. Then I create functions to downloade the annual reports, and extract the employee data. Finally, I plot the results. Overview of the process: 1) data = get_regnskap(“990412987”) # API data regnskap = regnskap_to_df(data) 2) download_annual_reports(“990412987”, 2020, 2025) # PDFs + OCR if needed 3) df_all = collect_employees(“reports/990412987”) # LLM extracts employees 4) plot_employees(df_all, title=“Stavanger Aftenblad”) Let’s first create functions for everything we are going to do. import os import io import re import time import shutil import subprocess from pathlib import Path from typing import Optional, List, Iterable, Tuple, Union import requests import pandas as pd import matplotlib.pyplot as plt from google.colab import userdata # PDF parsing try: from pypdf import PdfReader, PdfWriter except Exception: try: from PyPDF2 import PdfReader, PdfWriter # fallback except Exception: PdfReader = None PdfWriter = None # LLM + PDF loading from pydantic import BaseModel, Field from langchain_openai import AzureChatOpenAI from langchain_community.document_loaders import PyPDFLoader from langchain_core.prompts import ChatPromptTemplate # --------- # Config # --------- BASE_URL = &quot;https://data.brreg.no/regnskapsregisteret/regnskap/aarsregnskap/kopi/{orgnr}/{year}&quot; USER_AGENT = &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 &quot;\\ &quot;(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36&quot; # ------------- # Utilities # ------------- def sanitize_orgnr(orgnr: str) -&gt; str: digits = re.sub(r&quot;\\D+&quot;, &quot;&quot;, orgnr) if len(digits) &lt; 7: raise ValueError(f&quot;orgnr looks wrong: {orgnr!r}&quot;) return digits def ensure_dir(path: Union[str, Path]) -&gt; Path: p = Path(path) p.mkdir(parents=True, exist_ok=True) return p def have(cmd: str) -&gt; bool: return shutil.which(cmd) is not None def http_get_pdf(url: str, timeout: int = 30) -&gt; bytes: headers = { &quot;User-Agent&quot;: USER_AGENT, &quot;Accept&quot;: &quot;application/pdf,application/octet-stream;q=0.9,*/*;q=0.8&quot;, &quot;Accept-Language&quot;: &quot;nb-NO,nb;q=0.9,no;q=0.8,en-US;q=0.7,en;q=0.6&quot;, } r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, stream=True) ctype = (r.headers.get(&quot;Content-Type&quot;) or &quot;&quot;).lower() if r.status_code == 200 and (&quot;pdf&quot; in ctype or &quot;octet-stream&quot; in ctype or ctype == &quot;&quot;): return r.content r.raise_for_status() return r.content # defensive def has_extractable_text(pdf_path: Union[str, Path], max_pages: int = 3) -&gt; bool: if PdfReader is None: return False try: reader = PdfReader(str(pdf_path)) n = len(getattr(reader, &quot;pages&quot;, [])) to_check = min(n, max_pages) if n else max_pages for i in range(to_check): try: txt = reader.pages[i].extract_text() or &quot;&quot; except Exception: txt = &quot;&quot; if txt.strip(): return True return False except Exception: return False def ocr_inplace(pdf_path: Union[str, Path], lang: str = &quot;nor+eng&quot;) -&gt; None: &quot;&quot;&quot; If the PDF has no text, run OCR. Prefer ocrmypdf; otherwise a simple pytesseract fallback. Mutates the file in place. &quot;&quot;&quot; pdf_path = str(pdf_path) if has_extractable_text(pdf_path): return if have(&quot;ocrmypdf&quot;): tmp_out = pdf_path + &quot;.ocr.tmp.pdf&quot; cmd = [&quot;ocrmypdf&quot;, &quot;-l&quot;, lang, &quot;--skip-text&quot;, &quot;--output-type&quot;, &quot;pdf&quot;, pdf_path, tmp_out] try: subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) os.replace(tmp_out, pdf_path) return finally: if os.path.exists(tmp_out): try: os.remove(tmp_out) except: pass # Fallback (best-effort) try: from pdf2image import convert_from_path from PIL import Image # noqa: F401 import pytesseract except Exception: # No fallback libs → just return un-OCR&#39;d return if PdfReader is None or PdfWriter is None: return try: images = convert_from_path(pdf_path, dpi=300) writer = PdfWriter() for img in images: page_bytes = pytesseract.image_to_pdf_or_hocr(img, extension=&quot;pdf&quot;, lang=lang) temp_reader = PdfReader(io.BytesIO(page_bytes)) for p in temp_reader.pages: writer.add_page(p) with open(pdf_path, &quot;wb&quot;) as f: writer.write(f) except Exception: # swallow; keep original file pass def download_annual_reports( orgnr: str, year_start: int, year_end: int, out_dir: Optional[Union[str, Path]] = None, delay_s: float = 0.5, do_ocr: bool = True, ocr_lang: str = &quot;nor+eng&quot;, ) -&gt; Path: &quot;&quot;&quot; Download Brreg årsrapporter PDFs for orgnr in [year_start, year_end] inclusive. If `do_ocr=True`, run OCR when no extractable text is found. Returns the folder path containing the PDFs. &quot;&quot;&quot; org = sanitize_orgnr(orgnr) a, b = sorted([int(year_start), int(year_end)]) folder = ensure_dir(out_dir or Path(&quot;reports&quot;) / org) for year in range(a, b + 1): url = BASE_URL.format(orgnr=org, year=year) out_path = folder / f&quot;{org}_{year}.pdf&quot; if out_path.exists() and out_path.stat().st_size &gt; 0: if do_ocr: ocr_inplace(out_path, lang=ocr_lang) continue try: pdf_bytes = http_get_pdf(url) with open(out_path, &quot;wb&quot;) as f: f.write(pdf_bytes) if do_ocr: ocr_inplace(out_path, lang=ocr_lang) time.sleep(max(0.0, delay_s)) except Exception: # keep going; skip failures silently continue return folder # -------------------------------- # LLM: extract employees from PDF # -------------------------------- class EmployeeInfo(BaseModel): employees: Optional[int] = Field( default=None, description=&quot;Total headcount (integer). If absent, use FTE if present.&quot; ) page_numbers: List[int] = Field(default_factory=list, description=&quot;1-based pages with evidence.&quot;) confidence: float = Field(..., ge=0.0, le=1.0) rationale: Optional[str] = None raw_evidence: Optional[str] = None SYSTEM = &quot;&quot;&quot;You extract facts from text with care. Task: determine the company&#39;s total number of employees (headcount). If multiple values exist, prefer the most recent total headcount. Accept language variants: ansatte, antall ansatte, årsverk (FTE), staff, headcount. If only FTE is present, use it and note this in rationale. Return the specified schema. If unknown, employees=null, but provide confidence and rationale.&quot;&quot;&quot; USER_TMPL = &quot;&quot;&quot;You are given OCR/parsed text from a PDF annual report. Extract the number of employees (prefer headcount; use FTE only if headcount is missing). --- BEGIN TEXT (page-tagged) --- {page_tagged_text} --- END TEXT --- Rules: - If multiple values exist, choose the latest overall figure. - Include the 1-based page numbers where the evidence appears. &quot;&quot;&quot; def _select_relevant_pages( pages: Iterable, keywords: Iterable[str] = ( &quot;employee&quot;, &quot;employees&quot;, &quot;staff&quot;, &quot;ansatt&quot;, &quot;ansatte&quot;, &quot;antall ansatte&quot;, &quot;bemanning&quot;, &quot;årsverk&quot;, &quot;fte&quot;, &quot;headcount&quot; ), fallback_first_n: int = 5, ) -&gt; List[Tuple[int, str]]: keys = {k.lower() for k in keywords} sel: List[Tuple[int, str]] = [] for i, page in enumerate(pages, start=1): txt = (page.page_content or &quot;&quot;) low = txt.lower() if any(k in low for k in keys): sel.append((i, txt)) if not sel: # fallback to first N sel = [(i, (p.page_content or &quot;&quot;)) for i, p in enumerate(pages[:fallback_first_n], start=1)] return sel def _make_page_tagged(selected_pages: List[Tuple[int, str]], per_page_limit: int = 8000) -&gt; str: chunks = [] for i, txt in selected_pages: s = (txt or &quot;&quot;).strip() if len(s) &gt; per_page_limit: s = s[:per_page_limit] chunks.append(f&quot;[Page {i}]\\n{s}&quot;) return &quot;\\n\\n&quot;.join(chunks) def extract_employees_from_pdf(pdf_path: Union[str, Path], model: str = &quot;gpt-5-mini&quot;, temperature: float = 0.0) -&gt; EmployeeInfo: loader = PyPDFLoader(str(pdf_path)) docs = loader.load() selected = _select_relevant_pages(docs) page_tagged_text = _make_page_tagged(selected) client = AzureChatOpenAI( api_key=userdata.get(&quot;AZURE_OPENAI_API_KEY&quot;), azure_endpoint=&quot;https://gpt-ban443-1.openai.azure.com/&quot;, api_version=&quot;2025-01-01-preview&quot;, azure_deployment=&quot;Group01&quot;, temperature=0, ) structured_llm = client.with_structured_output(EmployeeInfo) prompt = ChatPromptTemplate.from_messages([(&quot;system&quot;, SYSTEM), (&quot;user&quot;, USER_TMPL)]) result: EmployeeInfo = (prompt | structured_llm).invoke({&quot;page_tagged_text&quot;: page_tagged_text}) return result YEAR_RE = re.compile(r&quot;(?&lt;!\\d)(20\\d{2})(?!\\d)&quot;) # 2000–2099 def _infer_year_from_name(path: Union[str, Path]) -&gt; Optional[int]: m = YEAR_RE.search(Path(path).stem) return int(m.group(1)) if m else None def collect_employees( company_folder: Union[str, Path], years: Optional[Union[int, List[int]]] = None, model: str = &quot;gpt-5-mini&quot;, temperature: float = 0.0, ) -&gt; pd.DataFrame: &quot;&quot;&quot; Run employee extraction over all PDFs in folder. Returns a tidy DataFrame with one row per PDF. &quot;&quot;&quot; company_folder = Path(company_folder) pdfs = sorted(company_folder.glob(&quot;*.pdf&quot;)) if not pdfs: return pd.DataFrame(columns=[&quot;company_id&quot;,&quot;year&quot;,&quot;employees&quot;,&quot;page_numbers&quot;,&quot;confidence&quot;,&quot;rationale&quot;,&quot;raw_evidence&quot;,&quot;source_path&quot;]) target_years = None if isinstance(years, int): target_years = {years} elif isinstance(years, list): target_years = set(map(int, years)) rows = [] company_id = company_folder.name for pdf in pdfs: yr = _infer_year_from_name(pdf) if target_years is not None and (yr is None or yr not in target_years): continue try: info = extract_employees_from_pdf(pdf, model=model, temperature=temperature) rows.append({ &quot;company_id&quot;: company_id, &quot;year&quot;: yr, &quot;employees&quot;: info.employees, &quot;page_numbers&quot;: info.page_numbers, &quot;confidence&quot;: info.confidence, &quot;rationale&quot;: info.rationale, &quot;raw_evidence&quot;: info.raw_evidence, &quot;source_path&quot;: str(pdf), }) except Exception: rows.append({ &quot;company_id&quot;: company_id, &quot;year&quot;: yr, &quot;employees&quot;: None, &quot;page_numbers&quot;: [], &quot;confidence&quot;: 0.0, &quot;rationale&quot;: &quot;extraction failed&quot;, &quot;raw_evidence&quot;: None, &quot;source_path&quot;: str(pdf), }) return pd.DataFrame.from_records(rows) # ------------------------- # Brreg Regnskap (API) # ------------------------- def get_regnskap(orgnr: str, year: Optional[int] = None, regnskapstype: str = &quot;SELSKAP&quot;) -&gt; list: &quot;&quot;&quot; Fetch regnskap JSON list from Brønnøysundregisteret. If year is None, API typically returns latest (and sometimes history). &quot;&quot;&quot; org = sanitize_orgnr(orgnr) base_url = f&quot;https://data.brreg.no/regnskapsregisteret/regnskap/{org}&quot; params = {&quot;regnskapstype&quot;: regnskapstype} if year is not None: params[&quot;år&quot;] = int(year) # note &#39;å&#39; r = requests.get(base_url, headers={&quot;accept&quot;: &quot;application/json&quot;}, params=params, timeout=30) r.raise_for_status() return r.json() def regnskap_to_df(data: list) -&gt; pd.DataFrame: recs = [] for item in data: orgnr = item[&quot;virksomhet&quot;][&quot;organisasjonsnummer&quot;] year = pd.to_datetime(item[&quot;regnskapsperiode&quot;][&quot;tilDato&quot;]).year R = item.get(&quot;resultatregnskapResultat&quot;, {}) or {} DR = R.get(&quot;driftsresultat&quot;, {}) or {} FR = R.get(&quot;finansresultat&quot;, {}) or {} recs.append({ &quot;orgnr&quot;: orgnr, &quot;year&quot;: year, &quot;regnskapstype&quot;: item.get(&quot;regnskapstype&quot;), &quot;valuta&quot;: item.get(&quot;valuta&quot;), &quot;sumEiendeler&quot;: (item.get(&quot;eiendeler&quot;) or {}).get(&quot;sumEiendeler&quot;), &quot;sumEgenkapitalGjeld&quot;: (item.get(&quot;egenkapitalGjeld&quot;) or {}).get(&quot;sumEgenkapitalGjeld&quot;), &quot;sumDriftsinntekter&quot;: (DR.get(&quot;driftsinntekter&quot;) or {}).get(&quot;sumDriftsinntekter&quot;), &quot;sumDriftskostnad&quot;: (DR.get(&quot;driftskostnad&quot;) or {}).get(&quot;sumDriftskostnad&quot;), &quot;driftsresultat&quot;: DR.get(&quot;driftsresultat&quot;), &quot;nettoFinans&quot;: FR.get(&quot;nettoFinans&quot;), &quot;ordinaertResultatFoerSkattekostnad&quot;: R.get(&quot;ordinaertResultatFoerSkattekostnad&quot;), &quot;aarsresultat&quot;: R.get(&quot;aarsresultat&quot;), }) return pd.DataFrame.from_records(recs) if recs else pd.DataFrame() # ------------------------- # Plot helper # ------------------------- def plot_employees(df: pd.DataFrame, title: str = &quot;Employees over time&quot;) -&gt; None: if df.empty: print(&quot;No employee data to plot.&quot;) return d = df.dropna(subset=[&quot;year&quot;, &quot;employees&quot;]).sort_values(&quot;year&quot;) if d.empty: print(&quot;No valid (year, employees) rows to plot.&quot;) return plt.figure(figsize=(8, 5)) plt.plot(d[&quot;year&quot;], d[&quot;employees&quot;], marker=&quot;o&quot;, linestyle=&quot;-&quot;, linewidth=2) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Number of Employees&quot;) plt.title(title) plt.xticks(d[&quot;year&quot;]) plt.grid(True, linestyle=&quot;--&quot;, alpha=0.6) plt.show() Now, it’s as simple as running the following code. First, to get the data from the open API. org = &quot;990412987&quot; data = get_regnskap(org) # latest or list regnskap = regnskap_to_df(data) print(regnskap) Then, to download the annual reports and extract the employee data. Note that this part is slow, so it might take a while to run. Reduce the numbers of years to 2021-2022 if you want to run it faster. folder = download_annual_reports(org, 2020, 2025) # PDFs saved to reports/&lt;org&gt;/ df_all = collect_employees(folder) print(df_all) Finally, to plot the results. plot_employees(df_all, title=&quot;Stavanger Aftenblad&quot;) "],["exercises.html", "Chapter 6 Exercises 6.1 Lab 1 6.2 Lab 2", " Chapter 6 Exercises 6.1 Lab 1 API Integration Exercise: Find an API online: a) get it to work in Python, b) create a function that uses it and c) show that it works. Course Chatbot: Make a chatbot to help with this course. Include course specific information and make sure to include a way to a) shut it down and b) restart it. Letterboxd Movie Classification: Use the Letterboxd API/dataset shown in the notebook. For a random subsample of 10 movies: Make the LLM classify the movies into movies you probably like/don’t like. Extract a few variables from the text that you might find interesting. Bonus - Dataset Analysis: Find a dataset online (for example Kaggle) and use the LangChain package to analyze it. You are free to use whatever functionality in LangChain you want. Bonus - Reasoning Model: Create a reasoning model using the non-reasoning LLM through the API. 6.2 Lab 2 Journalist Chatbot: Make a chatbot that helps a journalist to make a summary of their article. Make it as a function that can be called with a text input and returns a summary (easy). Dataset Analysis: Find a dataset on Kaggle and use the LangChain package to analyze it. You can find CSV datasets with text data here: https://www.kaggle.com/datasets?search=text&amp;fileType=csv&amp;tags=13204-NLP Import it using the pandas package. I.e. df = pd.read_csv(\"path/to/dataset.csv\") Use the LangChain package to analyze it. Extract some variables from the text data using the LLM or classify it into categories. Try to extract more than one variable from each row and store it back in the dataframe. Chatbot: Make a chatbot to help some spesific topic, making sure to include lots of extra information using RAG (more difficult). Term Paper: Start working on the term paper once you are done! "],["final-words.html", "Chapter 7 Final Words", " Chapter 7 Final Words This notebook provides an overview if the technical content of BAN443, starting with the basics of Python and LLMs, and then moving on to more advanced topics such as working with APIs, LangChain, and LangSmith. Learning coding is easiest when you work through the examples and exercises yourself. Good luck! "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
