# Applications

This chapter demonstrates real-world applications of Large Language Models (LLMs) using the LangChain framework. We'll explore various use cases ranging from data extraction from the Norwegian National Library to political analysis, web search integration, RAG systems, and automated company data extraction from Brønnøysundregisteret.

All applications in this chapter use LangChain for their implementation, showcasing the framework's capabilities for structured outputs, document processing, agent systems, and retrieval-augmented generation.

Note that you might want to mount your Google Drive to Colab to save the results of your applications.

```python
from google.colab import drive
drive.mount('/content/drive')
```

This will mount your Google Drive at `/content/drive`. You can now access your files using the path `/content/drive/MyDrive/`.

## Data Extraction from the Norwegian National Library

The Norwegian National Library provides access to digitized historical documents through their API. This section demonstrates how to extract and structure information from these documents using LangChain.

### Setting up LangChain with Azure OpenAI

```python
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Optional, List

# Initialize LangChain with Azure OpenAI
client = AzureChatOpenAI(
    api_key=userdata.get("AZURE_OPENAI_API_KEY"),
    azure_endpoint="https://gpt-ban443-1.openai.azure.com/",
    api_version="2025-01-01-preview",
    azure_deployment="Group01",
    temperature=0,
)
```

### Structured Data Extraction

```python
# Define Pydantic models for structured extraction
class entity(BaseModel):
    """Structure data about a person from noisy text."""
    surname: Optional[str] = Field(
        default=None, 
        description="The surname of a person. A dash indicates that the surname is same as last person (indicate this with '-')"
    )
    firstname: Optional[str] = Field(
        default=None, 
        description="The firstname of a person. Listed after a surname and a comma or a dash."
    )
    occupation: Optional[str] = Field(
        default=None, 
        description="The occupation of that person, not including firm names in parenthesis."
    )
    firm: Optional[str] = Field(
        default=None, 
        description="The firm that a person works at. It is included in parentheses (if at all)."
    )
    address: Optional[str] = Field(
        default=None, 
        description="The place of residence for the person"
    )

class entities(BaseModel):
    """Identifying information about all people in a text."""
    people: List[entity]

# Create extraction prompt template
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """You are an expert at identifying information on individuals from address books.
        Only extract information on individuals and ignore what appears to be noise.
        Extract nothing if no important information can be found in the text.
        The content you see is from OCR scans of historical documents and will be noisy,
        so if there are obvious errors you can correct them in your response.
        If surname is indicated with a dash ('-'), then include this as surname, the rest of the name is in firstname.
        All names should start with upper case letters, while occupations start with lower case letters.

        Firm names in parenthesis should not be included in the occupation, but in the firm variable.
        Everyone should have a firstname.

        A typical observation in the book are like this:
        Berger, Eirik, forsker (SSB), Oscars gt. 999
        """,
    ),
    ("human", "{text}"),
])

# Create structured extraction chain
extractor = prompt | client.with_structured_output(
    schema=entities,
    include_raw=True,
)

# Extract information from text
response = extractor.invoke("My name is Eirik Abel, and I was born in 1993. I am a student. There is another person named Barne Ollson who is 34 years old and lives in Bergen")
print(response['parsed'].people[0].model_dump_json())
```

### Fetching Data from the Norwegian National Library

```python
!pip install wget
!pip install PyMuPDF

import wget
import fitz
from langchain_text_splitters import CharacterTextSplitter

# Define the page range you want to extract from the National Library
start_page = 500
end_page = 520

# Downloading a file from the Norwegian National Library
url = f"https://www.nb.no/services/downloader?urn=URN:NBN:no-nb_digitidsskrift_2019091381014_001&resolutionlevel=4&pg={start_page}-{end_page}&text=true&filename=2024-10-06T1216_NB_generated"
wget.download(url, "address_book.pdf")

# Extract text from PDF
doc = fitz.open("address_book.pdf")
all_text = ""
for page_num in range(0, end_page - start_page):
    page = doc.load_page(page_num)
    all_text += page.get_text("text")

# Split text into manageable chunks
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=500,
    chunk_overlap=0,
    length_function=len,
    is_separator_regex=False,
)

texts = text_splitter.create_documents([all_text])
print(f"Created {len(texts)} text chunks from National Library data")
```

### Batch Processing with Caching

Setting up caching. This means that the LLM will remember the results of the previous calls, so that it doesn't have to re-compute the same thing.

```python
# Cache setup.
# https://python.langchain.com/docs/integrations/llm_caching/
from langchain.globals import set_llm_cache

## Using in memory cache:
from langchain_community.cache import InMemoryCache
set_llm_cache(InMemoryCache())

## OR use SQLite cache:
# from langchain_community.cache import SQLiteCache
# set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# Use either memory cache or SQLite cache.
```

Process 10 random chunks of text in parallel:

```python
import random
random.seed(100)
random_elements = random.sample(texts, 10)

print("Processing documents with caching enabled...")
extractions = extractor.batch(
    [{"text": text.page_content} for text in random_elements],
    {"max_concurrency": 5},
)
print(f"Completed processing {len(extractions)} documents")
```

### Converting JSON Results to DataFrame

Lets convert results from the batch results to a dataframe. We use the result from the batch extraction above. Note that this is the kind of tasks that chatgpt can help you produce code for if you just give it the object you want to convert.

```python	
# For batch results, process all extractions
df_list = []
for index, result in enumerate(extractions):
    try:
        # Get the parsed JSON data
        json_data = result['parsed'].model_dump_json()
        parsed_data = json.loads(json_data)
        
        # Convert to DataFrame
        df = pd.json_normalize(parsed_data['people'])
        df['file_index'] = index
        df_list.append(df)
    except Exception as e:
        print(f"Error processing document {index}: {e}")

# Combine all results
combined_df = pd.concat(df_list, ignore_index=True)
print(f"\nCombined DataFrame shape: {combined_df.shape}")
print("\nFirst few rows:")
print(combined_df.head())
```

Second, lets convert the response from a single element/answer to a dataframe.

```python
# Import
import json
import pandas as pd

# Simple example: Convert a single extraction result to DataFrame
sample_result = extractor.invoke("Berger, Eirik, forsker (SSB), Oscars gt. 999")

# Extract the JSON data
json_data = sample_result['parsed'].model_dump_json()
parsed_data = json.loads(json_data)

# Convert to DataFrame
df = pd.json_normalize(parsed_data['people'])
print("Sample DataFrame:")
print(df)
```

### Analyzing Occupation Data

```python
import matplotlib.pyplot as plt # helps make the figure

# Clean and analyze occupation data
occupation_data = combined_df.dropna(subset=['occupation'])
occupation_counts = occupation_data['occupation'].value_counts().head(10)

print("Top 10 Most Common Occupations:")
print(occupation_counts)

# Create visualization
plt.figure(figsize=(12, 8))
occupation_counts.plot(kind='bar', color='skyblue', edgecolor='navy', alpha=0.7)
plt.title('Top 10 Most Common Occupations from Norwegian National Library Data', 
          fontsize=14, fontweight='bold')
plt.xlabel('Occupation', fontsize=12)
plt.ylabel('Number of People', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Additional analysis: Geographic distribution
if 'address' in combined_df.columns:
    address_data = combined_df.dropna(subset=['address'])
    print(f"\nGeographic distribution (top 10):")
    print(address_data['address'].value_counts().head(10))
```

## Web Search and Tools Integration

LangChain provides powerful tools for integrating web search and other external APIs into LLM workflows. We first need to get an API key for Tavily from their website (https://www.tavily.com/). 

```python	
import os
from google.colab import userdata

!pip install langchain_tavily

os.environ["TAVILY_API_KEY"] = userdata.get('tavily') # This is where you include your API key!
```

### Basic Web Search with Tavily

```python
from langchain_tavily import TavilySearch

# Initialize search tool
tool = TavilySearch(max_results=2)
tools = [tool]

# Perform search
search_results = tool.invoke("What was the result of the football match between Norway and Moldova?")
print(search_results)
```

### Agent with Web Search Capabilities

We will now use the langgraph package to work with more advanced agentic systems. LangGraph is particularly useful for building conversational agents, RAG systems, and complex multi-step reasoning applications. Most of the examples where we use LangGraph is taken from their website (https://langchain-ai.github.io/langgraph/). Start by installing it. 

```python
!pip install langgraph
```

Then, we can use the following code to build an agent with web search capabilities.	

```python
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from typing import Annotated
from typing_extensions import TypedDict

class State(TypedDict):
    messages: Annotated[list, add_messages]

# Create agent with search capabilities
graph_builder = StateGraph(State)

tool = TavilySearch(max_results=2)
tools = [tool]
llm_with_tools = client.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()

# Use the agent
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

# Example usage
stream_graph_updates("What's the latest news about NHH?")
```

## Retrieval-Augmented Generation (RAG) Systems

Next we want to create a RAG system. We often want to give our LLMs more knowledge and data to work with. This can be done by 1) Training the model with more data, 2) Including data directly in the prompt, or 3) Using a RAG system. As option 1 is often expensive and option 2 is often too limiting, option 3 has become popular. Note however that we need an embedding model to make this work, which we don't have an API key for this class yet. You can create your own API key at https://platform.openai.com/ if you want to try it out, but note that it cost money depending on how much you use it (although not much). 


### Basic RAG Implementation

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langgraph.graph import START, StateGraph, END
from typing_extensions import List, TypedDict
from langchain_core.documents import Document
import bs4
from langchain import hub

# Setup embeddings and vector store
client = ChatOpenAI(api_key=userdata.get('new_openai'), model="gpt-4o-mini")
embeddings = OpenAIEmbeddings(api_key=userdata.get('new_openai'))
vector_store = InMemoryVectorStore(embeddings)

# Load and chunk documents
loader = WebBaseLoader(
    web_paths=("https://nhhs.no/nhhs-fra-a-til-a/","https://nhhs.no/interessegrupper/", "https://www.dn.no/"),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)

# Index chunks
_ = vector_store.add_documents(documents=all_splits)

# Define prompt for question-answering
# Pull a pre-built RAG prompt template from LangChain Hub
# This prompt is specifically designed for retrieval-augmented generation (RAG) tasks
# It includes placeholders for question and context that will be filled in during execution
prompt = hub.pull("rlm/rag-prompt")

# Define RAG state
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

# Define RAG functions
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}

def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = client.invoke(messages)
    return {"answer": response.content}

# Build RAG graph
builder = StateGraph(State)
builder.add_node("retrieve", retrieve)
builder.add_node("generate", generate)
builder.add_edge(START, "retrieve")
builder.add_edge("retrieve", "generate")
builder.add_edge("generate", END)
graph = builder.compile()

# Use RAG system
response = graph.invoke({"question": "What's 'ASAP'? Answer in english."})
print(response["answer"])
```

### Persistent Vector Database
The last example created a vector database in memory, which will dissapear once we end our notebook session. We can instead save the database to a file, so that it persists between sessions.

```python
!pip install -U langchain-chroma chromadb
from langchain_community.vectorstores import Chroma

# Choose a stable collection name + persist dir
collection_name = "nhhs_docs"
persist_dir = "./chroma_db"

# --- Create / upsert (auto-persisted; no .persist() needed) ---
vector_store = Chroma.from_documents(
    documents=all_splits,
    embedding=embeddings,
    collection_name=collection_name,
    persist_directory=persist_dir,
)

# --- Load an existing DB later ---
vector_store = Chroma(
    collection_name=collection_name,
    persist_directory=persist_dir,
    embedding_function=embeddings,
)

# --- Query or use it in the RAG system ---
results = vector_store.similarity_search("NHH student groups", k=3)
for doc in results:
    print(f"Content: {doc.page_content[:200]}...")
    print(f"Source: {doc.metadata.get('source', 'Unknown')}")
    print("---")
```

## Company Data Extraction from Brønnøysundregisteret

This section demonstrates automated extraction of financial and employee data from company annual reports using LangChain's structured output capabilities. This is a potentially very valuable application in a long range of sectors. This is quite complex compared that what we have previously looked at, but it's not expected that you understand all of this code. Its mainly a showcase of what is possible with LLMs, and if you would want to do something similar, then you could use this as a starting point.

I first get data from their open API. Then I create functions to downloade the annual reports, and extract the employee data. Finally, I plot the results.

Overview of the process:
  1) data = get_regnskap("990412987")        # API data
     regnskap = regnskap_to_df(data)
  2) download_annual_reports("990412987", 2020, 2025)  # PDFs + OCR if needed
  3) df_all = collect_employees("reports/990412987")   # LLM extracts employees
  4) plot_employees(df_all, title="Stavanger Aftenblad")

Let's first create functions for everything we are going to do.
```python
import os
import io
import re
import time
import shutil
import subprocess
from pathlib import Path
from typing import Optional, List, Iterable, Tuple, Union

import requests
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import userdata

# PDF parsing
try:
    from pypdf import PdfReader, PdfWriter
except Exception:
    try:
        from PyPDF2 import PdfReader, PdfWriter  # fallback
    except Exception:
        PdfReader = None
        PdfWriter = None

# LLM + PDF loading
from pydantic import BaseModel, Field
from langchain_openai import AzureChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate

# ---------
# Config
# ---------
BASE_URL = "https://data.brreg.no/regnskapsregisteret/regnskap/aarsregnskap/kopi/{orgnr}/{year}"
USER_AGENT = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "\
             "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# -------------
# Utilities
# -------------
def sanitize_orgnr(orgnr: str) -> str:
    digits = re.sub(r"\D+", "", orgnr)
    if len(digits) < 7:
        raise ValueError(f"orgnr looks wrong: {orgnr!r}")
    return digits

def ensure_dir(path: Union[str, Path]) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p

def have(cmd: str) -> bool:
    return shutil.which(cmd) is not None

def http_get_pdf(url: str, timeout: int = 30) -> bytes:
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/pdf,application/octet-stream;q=0.9,*/*;q=0.8",
        "Accept-Language": "nb-NO,nb;q=0.9,no;q=0.8,en-US;q=0.7,en;q=0.6",
    }
    r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, stream=True)
    ctype = (r.headers.get("Content-Type") or "").lower()
    if r.status_code == 200 and ("pdf" in ctype or "octet-stream" in ctype or ctype == ""):
        return r.content
    r.raise_for_status()
    return r.content  # defensive

def has_extractable_text(pdf_path: Union[str, Path], max_pages: int = 3) -> bool:
    if PdfReader is None:
        return False
    try:
        reader = PdfReader(str(pdf_path))
        n = len(getattr(reader, "pages", []))
        to_check = min(n, max_pages) if n else max_pages
        for i in range(to_check):
            try:
                txt = reader.pages[i].extract_text() or ""
            except Exception:
                txt = ""
            if txt.strip():
                return True
        return False
    except Exception:
        return False

def ocr_inplace(pdf_path: Union[str, Path], lang: str = "nor+eng") -> None:
    """
    If the PDF has no text, run OCR. Prefer ocrmypdf; otherwise a simple pytesseract fallback.
    Mutates the file in place.
    """
    pdf_path = str(pdf_path)
    if has_extractable_text(pdf_path):
        return

    if have("ocrmypdf"):
        tmp_out = pdf_path + ".ocr.tmp.pdf"
        cmd = ["ocrmypdf", "-l", lang, "--skip-text", "--output-type", "pdf", pdf_path, tmp_out]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            os.replace(tmp_out, pdf_path)
            return
        finally:
            if os.path.exists(tmp_out):
                try: os.remove(tmp_out)
                except: pass

    # Fallback (best-effort)
    try:
        from pdf2image import convert_from_path
        from PIL import Image  # noqa: F401
        import pytesseract
    except Exception:
        # No fallback libs → just return un-OCR'd
        return

    if PdfReader is None or PdfWriter is None:
        return

    try:
        images = convert_from_path(pdf_path, dpi=300)
        writer = PdfWriter()
        for img in images:
            page_bytes = pytesseract.image_to_pdf_or_hocr(img, extension="pdf", lang=lang)
            temp_reader = PdfReader(io.BytesIO(page_bytes))
            for p in temp_reader.pages:
                writer.add_page(p)
        with open(pdf_path, "wb") as f:
            writer.write(f)
    except Exception:
        # swallow; keep original file
        pass

def download_annual_reports(
    orgnr: str,
    year_start: int,
    year_end: int,
    out_dir: Optional[Union[str, Path]] = None,
    delay_s: float = 0.5,
    do_ocr: bool = True,
    ocr_lang: str = "nor+eng",
) -> Path:
    """
    Download Brreg årsrapporter PDFs for orgnr in [year_start, year_end] inclusive.
    If `do_ocr=True`, run OCR when no extractable text is found.
    Returns the folder path containing the PDFs.
    """
    org = sanitize_orgnr(orgnr)
    a, b = sorted([int(year_start), int(year_end)])
    folder = ensure_dir(out_dir or Path("reports") / org)

    for year in range(a, b + 1):
        url = BASE_URL.format(orgnr=org, year=year)
        out_path = folder / f"{org}_{year}.pdf"
        if out_path.exists() and out_path.stat().st_size > 0:
            if do_ocr:
                ocr_inplace(out_path, lang=ocr_lang)
            continue

        try:
            pdf_bytes = http_get_pdf(url)
            with open(out_path, "wb") as f:
                f.write(pdf_bytes)
            if do_ocr:
                ocr_inplace(out_path, lang=ocr_lang)
            time.sleep(max(0.0, delay_s))
        except Exception:
            # keep going; skip failures silently
            continue

    return folder

# --------------------------------
# LLM: extract employees from PDF
# --------------------------------
class EmployeeInfo(BaseModel):
    employees: Optional[int] = Field(
        default=None,
        description="Total headcount (integer). If absent, use FTE if present."
    )
    page_numbers: List[int] = Field(default_factory=list, description="1-based pages with evidence.")
    confidence: float = Field(..., ge=0.0, le=1.0)
    rationale: Optional[str] = None
    raw_evidence: Optional[str] = None

SYSTEM = """You extract facts from text with care.
Task: determine the company's total number of employees (headcount).
If multiple values exist, prefer the most recent total headcount.
Accept language variants: ansatte, antall ansatte, årsverk (FTE), staff, headcount.
If only FTE is present, use it and note this in rationale.
Return the specified schema. If unknown, employees=null, but provide confidence and rationale."""

USER_TMPL = """You are given OCR/parsed text from a PDF annual report.
Extract the number of employees (prefer headcount; use FTE only if headcount is missing).

--- BEGIN TEXT (page-tagged) ---
{page_tagged_text}
--- END TEXT ---

Rules:
- If multiple values exist, choose the latest overall figure.
- Include the 1-based page numbers where the evidence appears.
"""

def _select_relevant_pages(
    pages: Iterable,
    keywords: Iterable[str] = (
        "employee", "employees", "staff",
        "ansatt", "ansatte", "antall ansatte", "bemanning",
        "årsverk", "fte", "headcount"
    ),
    fallback_first_n: int = 5,
) -> List[Tuple[int, str]]:
    keys = {k.lower() for k in keywords}
    sel: List[Tuple[int, str]] = []
    for i, page in enumerate(pages, start=1):
        txt = (page.page_content or "")
        low = txt.lower()
        if any(k in low for k in keys):
            sel.append((i, txt))
    if not sel:
        # fallback to first N
        sel = [(i, (p.page_content or "")) for i, p in enumerate(pages[:fallback_first_n], start=1)]
    return sel

def _make_page_tagged(selected_pages: List[Tuple[int, str]], per_page_limit: int = 8000) -> str:
    chunks = []
    for i, txt in selected_pages:
        s = (txt or "").strip()
        if len(s) > per_page_limit:
            s = s[:per_page_limit]
        chunks.append(f"[Page {i}]\n{s}")
    return "\n\n".join(chunks)

def extract_employees_from_pdf(pdf_path: Union[str, Path], model: str = "gpt-5-mini", temperature: float = 0.0) -> EmployeeInfo:
    loader = PyPDFLoader(str(pdf_path))
    docs = loader.load()
    selected = _select_relevant_pages(docs)
    page_tagged_text = _make_page_tagged(selected)

    client = AzureChatOpenAI(
        api_key=userdata.get("AZURE_OPENAI_API_KEY"),
        azure_endpoint="https://gpt-ban443-1.openai.azure.com/",
        api_version="2025-01-01-preview",
        azure_deployment="Group01",
        temperature=0,
    )

    structured_llm = client.with_structured_output(EmployeeInfo)
    prompt = ChatPromptTemplate.from_messages([("system", SYSTEM), ("user", USER_TMPL)])
    result: EmployeeInfo = (prompt | structured_llm).invoke({"page_tagged_text": page_tagged_text})
    return result

YEAR_RE = re.compile(r"(?<!\d)(20\d{2})(?!\d)")  # 2000–2099

def _infer_year_from_name(path: Union[str, Path]) -> Optional[int]:
    m = YEAR_RE.search(Path(path).stem)
    return int(m.group(1)) if m else None

def collect_employees(
    company_folder: Union[str, Path],
    years: Optional[Union[int, List[int]]] = None,
    model: str = "gpt-5-mini",
    temperature: float = 0.0,
) -> pd.DataFrame:
    """
    Run employee extraction over all PDFs in folder.
    Returns a tidy DataFrame with one row per PDF.
    """
    company_folder = Path(company_folder)
    pdfs = sorted(company_folder.glob("*.pdf"))
    if not pdfs:
        return pd.DataFrame(columns=["company_id","year","employees","page_numbers","confidence","rationale","raw_evidence","source_path"])

    target_years = None
    if isinstance(years, int):
        target_years = {years}
    elif isinstance(years, list):
        target_years = set(map(int, years))

    rows = []
    company_id = company_folder.name
    for pdf in pdfs:
        yr = _infer_year_from_name(pdf)
        if target_years is not None and (yr is None or yr not in target_years):
            continue
        try:
            info = extract_employees_from_pdf(pdf, model=model, temperature=temperature)
            rows.append({
                "company_id": company_id,
                "year": yr,
                "employees": info.employees,
                "page_numbers": info.page_numbers,
                "confidence": info.confidence,
                "rationale": info.rationale,
                "raw_evidence": info.raw_evidence,
                "source_path": str(pdf),
            })
        except Exception:
            rows.append({
                "company_id": company_id,
                "year": yr,
                "employees": None,
                "page_numbers": [],
                "confidence": 0.0,
                "rationale": "extraction failed",
                "raw_evidence": None,
                "source_path": str(pdf),
            })
    return pd.DataFrame.from_records(rows)

# -------------------------
# Brreg Regnskap (API)
# -------------------------
def get_regnskap(orgnr: str, year: Optional[int] = None, regnskapstype: str = "SELSKAP") -> list:
    """
    Fetch regnskap JSON list from Brønnøysundregisteret.
    If year is None, API typically returns latest (and sometimes history).
    """
    org = sanitize_orgnr(orgnr)
    base_url = f"https://data.brreg.no/regnskapsregisteret/regnskap/{org}"
    params = {"regnskapstype": regnskapstype}
    if year is not None:
        params["år"] = int(year)  # note 'å'
    r = requests.get(base_url, headers={"accept": "application/json"}, params=params, timeout=30)
    r.raise_for_status()
    return r.json()

def regnskap_to_df(data: list) -> pd.DataFrame:
    recs = []
    for item in data:
        orgnr = item["virksomhet"]["organisasjonsnummer"]
        year = pd.to_datetime(item["regnskapsperiode"]["tilDato"]).year
        R = item.get("resultatregnskapResultat", {}) or {}
        DR = R.get("driftsresultat", {}) or {}
        FR = R.get("finansresultat", {}) or {}
        recs.append({
            "orgnr": orgnr,
            "year": year,
            "regnskapstype": item.get("regnskapstype"),
            "valuta": item.get("valuta"),
            "sumEiendeler": (item.get("eiendeler") or {}).get("sumEiendeler"),
            "sumEgenkapitalGjeld": (item.get("egenkapitalGjeld") or {}).get("sumEgenkapitalGjeld"),
            "sumDriftsinntekter": (DR.get("driftsinntekter") or {}).get("sumDriftsinntekter"),
            "sumDriftskostnad": (DR.get("driftskostnad") or {}).get("sumDriftskostnad"),
            "driftsresultat": DR.get("driftsresultat"),
            "nettoFinans": FR.get("nettoFinans"),
            "ordinaertResultatFoerSkattekostnad": R.get("ordinaertResultatFoerSkattekostnad"),
            "aarsresultat": R.get("aarsresultat"),
        })
    return pd.DataFrame.from_records(recs) if recs else pd.DataFrame()

# -------------------------
# Plot helper
# -------------------------
def plot_employees(df: pd.DataFrame, title: str = "Employees over time") -> None:
    if df.empty:
        print("No employee data to plot.")
        return
    d = df.dropna(subset=["year", "employees"]).sort_values("year")
    if d.empty:
        print("No valid (year, employees) rows to plot.")
        return
    plt.figure(figsize=(8, 5))
    plt.plot(d["year"], d["employees"], marker="o", linestyle="-", linewidth=2)
    plt.xlabel("Year")
    plt.ylabel("Number of Employees")
    plt.title(title)
    plt.xticks(d["year"])
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()
```

Now, it's as simple as running the following code. First, to get the data from the open API. 

```**python**
org = "990412987"
data = get_regnskap(org)          # latest or list
regnskap = regnskap_to_df(data)
print(regnskap)
```
Then, to download the annual reports and extract the employee data. Note that this part is slow, so it might take a while to run. Reduce the numbers of years to 2021-2022 if you want to run it faster.

```python
folder = download_annual_reports(org, 2020, 2025)   # PDFs saved to reports/<org>/
df_all = collect_employees(folder)
print(df_all)
```

Finally, to plot the results.

```python
plot_employees(df_all, title="Stavanger Aftenblad")
```

