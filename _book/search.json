[{"path":"index.html","id":"welcome-to-ai-and-large-language-models","chapter":"1 Welcome to AI and Large Language Models","heading":"1 Welcome to AI and Large Language Models","text":"Welcome comprehensive course Artificial Intelligence Large Language Models (LLMs). book designed take fundamentals Python programming advanced applications AI real-world scenarios.","code":""},{"path":"index.html","id":"course-overview","chapter":"1 Welcome to AI and Large Language Models","heading":"1.1 Course Overview","text":"course covers essential skills needed work modern AI systems, particularly Large Language Models. ’ll learn:Python Programming: Essential Python skills AI data analysisLLM Fundamentals: Understanding language models work hoodAPI Integration: Working commercial open-source LLM APIsLangChain Framework: Building sophisticated AI applicationsPractical Applications: Real-world projects including data scraping, analysis, automation","code":""},{"path":"index.html","id":"learning-objectives","chapter":"1 Welcome to AI and Large Language Models","heading":"1.2 Learning Objectives","text":"end course, able :Write Python code data manipulation analysisUnderstand technical foundations Large Language ModelsIntegrate LLM APIs applicationsUse LangChain framework complex AI workflowsBuild practical AI applications business researchApply AI techniques real-world data analysis problems","code":""},{"path":"index.html","id":"prerequisites","chapter":"1 Welcome to AI and Large Language Models","heading":"1.3 Prerequisites","text":"course assumes:Basic familiarity programming concepts (variables, functions, loops)Comfort using computers installing softwareNo prior experience AI machine learning requiredWillingness experiment learn hands-practice","code":""},{"path":"index.html","id":"course-structure","chapter":"1 Welcome to AI and Large Language Models","heading":"1.4 Course Structure","text":"course organized progressive chapters, building upon previous:Chapter 1: Introduction Python AIChapter 2: Understanding Large Language ModelsChapter 3: Working LLM APIsChapter 4: LangChain FrameworkChapter 5: Advanced LangChain CachingChapter 6: Practical Applications Case Studies","code":""},{"path":"index.html","id":"getting-started","chapter":"1 Welcome to AI and Large Language Models","heading":"1.5 Getting Started","text":"get course:Set Python environment (recommend Google Colab beginners)Create accounts required APIs (instructions provided chapter)Follow along code examplesComplete hands-exercisesExperiment variations examples","code":""},{"path":"index.html","id":"contact-and-support","chapter":"1 Welcome to AI and Large Language Models","heading":"1.6 Contact and Support","text":"questions course, please contact instructor eirik.berger@gmail.com ‘BAN443’ included subject line.","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction to Python for AI","heading":"2 Introduction to Python for AI","text":"chapter introduces essential Python programming concepts ’ll need work AI Large Language Models. focus course understanding applying AI technologies, solid Python fundamentals make everything else much easier.","code":""},{"path":"intro.html","id":"why-python-for-ai","chapter":"2 Introduction to Python for AI","heading":"2.1 Why Python for AI?","text":"Python become de facto language AI machine learning several reasons:Rich ecosystem: Extensive libraries data science, machine learning, AIReadable syntax: Easy learn understand, even beginnersStrong community: Large community excellent documentation supportIndustry standard: AI companies researchers use Python","code":""},{"path":"intro.html","id":"setting-up-your-environment","chapter":"2 Introduction to Python for AI","heading":"2.2 Setting Up Your Environment","text":"","code":""},{"path":"intro.html","id":"option-1-google-colab-recommended-for-beginners","chapter":"2 Introduction to Python for AI","heading":"2.2.1 Option 1: Google Colab (Recommended for Beginners)","text":"Google Colab provides free, cloud-based Python environment ’s perfect learning:Go colab.research.google.comSign Google accountCreate new notebookStart coding immediately!","code":""},{"path":"intro.html","id":"option-2-local-installation","chapter":"2 Introduction to Python for AI","heading":"2.2.2 Option 2: Local Installation","text":"prefer work locally, install Python Jupyter:","code":"# Install Python (if not already installed)\n# Download from python.org or use a package manager\n\n# Install Jupyter\npip install jupyter\n\n# Install required packages\npip install pandas numpy matplotlib requests transformers torch"},{"path":"intro.html","id":"essential-python-concepts","chapter":"2 Introduction to Python for AI","heading":"2.3 Essential Python Concepts","text":"","code":""},{"path":"intro.html","id":"variables-and-data-types","chapter":"2 Introduction to Python for AI","heading":"2.3.1 Variables and Data Types","text":"Python uses dynamic typing, meaning don’t need declare variable types:","code":"# Numbers\nage = 25\nheight = 5.9\nis_student = True\n\n# Strings\nname = \"Alice\"\nmessage = 'Hello, World!'\n\n# Lists (arrays)\nfruits = [\"apple\", \"banana\", \"orange\"]\nnumbers = [1, 2, 3, 4, 5]\n\n# Dictionaries (key-value pairs)\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 25,\n    \"city\": \"New York\"\n}"},{"path":"intro.html","id":"functions","chapter":"2 Introduction to Python for AI","heading":"2.3.2 Functions","text":"Functions allow organize reuse code:","code":"def greet(name):\n    \"\"\"A simple function that greets someone.\"\"\"\n    return f\"Hello, {name}!\"\n\n# Using the function\nmessage = greet(\"Alice\")\nprint(message)  # Output: Hello, Alice!"},{"path":"intro.html","id":"working-with-data","chapter":"2 Introduction to Python for AI","heading":"2.3.3 Working with Data","text":"AI work, ’ll frequently work structured data:","code":"import pandas as pd\n\n# Create a simple dataset\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35],\n    'city': ['New York', 'London', 'Tokyo']\n}\n\ndf = pd.DataFrame(data)\nprint(df)"},{"path":"intro.html","id":"error-handling","chapter":"2 Introduction to Python for AI","heading":"2.3.4 Error Handling","text":"Learning handle errors gracefully crucial:","code":"try:\n    # Code that might fail\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")"},{"path":"intro.html","id":"working-with-apis","chapter":"2 Introduction to Python for AI","heading":"2.4 Working with APIs","text":"APIs (Application Programming Interfaces) ’ll interact AI services:","code":"import requests\n\n# Example API call\nurl = \"https://api.example.com/data\"\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    data = response.json()\n    print(\"Success!\")\nelse:\n    print(f\"Error: {response.status_code}\")"},{"path":"intro.html","id":"key-libraries-for-ai-work","chapter":"2 Introduction to Python for AI","heading":"2.5 Key Libraries for AI Work","text":"essential Python libraries ’ll use throughout course:pandas: Data manipulation analysisnumpy: Numerical computingrequests: Making HTTP requests APIstransformers: Working pre-trained language modelstorch: Deep learning frameworklangchain: Framework building LLM applications","code":""},{"path":"intro.html","id":"best-practices","chapter":"2 Introduction to Python for AI","heading":"2.6 Best Practices","text":"Use meaningful variable names: user_age instead aAdd comments: Explain code doesHandle errors: Always include error handlingTest incrementally: Run code frequently catch errors earlyUse functions: Break complex tasks smaller functions","code":""},{"path":"intro.html","id":"next-steps","chapter":"2 Introduction to Python for AI","heading":"2.7 Next Steps","text":"Now Python basics, ’re ready dive fascinating world Large Language Models. next chapter, ’ll explore models work get hands-experience GPT-2.Remember: Don’t worry ’re Python expert yet. important thing start experimenting learning practice. chapter build skills progressively.","code":""},{"path":"llm-fundamentals.html","id":"llm-fundamentals","chapter":"3 Understanding Large Language Models","heading":"3 Understanding Large Language Models","text":"chapter, ’ll dive deep Large Language Models (LLMs) work hood. Understanding fundamentals help use LLMs effectively troubleshoot issues arise.","code":""},{"path":"llm-fundamentals.html","id":"what-are-large-language-models","chapter":"3 Understanding Large Language Models","heading":"3.1 What Are Large Language Models?","text":"Large Language Models AI systems trained vast amounts text data understand generate human-like language. core, sophisticated pattern recognition systems learn predict next word (token) sequence.","code":""},{"path":"llm-fundamentals.html","id":"key-characteristics","chapter":"3 Understanding Large Language Models","heading":"3.1.1 Key Characteristics","text":"Scale: Trained billions trillions parametersData: Trained massive text corpora internetArchitecture: Based transformer neural networksCapability: Can perform wide range language tasks","code":""},{"path":"llm-fundamentals.html","id":"the-transformer-architecture","chapter":"3 Understanding Large Language Models","heading":"3.2 The Transformer Architecture","text":"transformer architecture, introduced paper “Attention Need” (Vaswani et al., 2017), foundation modern LLMs:","code":""},{"path":"llm-fundamentals.html","id":"key-components","chapter":"3 Understanding Large Language Models","heading":"3.2.1 Key Components","text":"Attention Mechanism: Allows model focus relevant parts inputSelf-Attention: Enables model understand relationships wordsFeed-Forward Networks: Process attended informationLayer Normalization: Stabilizes training improves performance","code":""},{"path":"llm-fundamentals.html","id":"tokenization-from-text-to-numbers","chapter":"3 Understanding Large Language Models","heading":"3.3 Tokenization: From Text to Numbers","text":"LLMs can process text, must converted numbers. process called tokenization.","code":""},{"path":"llm-fundamentals.html","id":"how-tokenization-works","chapter":"3 Understanding Large Language Models","heading":"3.3.1 How Tokenization Works","text":"","code":"from transformers import GPT2Tokenizer\n\n# Load the GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Tokenize a word\ninputs = tokenizer(\"inequality\", return_tensors=\"pt\")\nprint(\"Token IDs:\", inputs.input_ids)\nprint(\"Token IDs as list:\", inputs.input_ids[0].tolist())"},{"path":"llm-fundamentals.html","id":"understanding-token-ids","chapter":"3 Understanding Large Language Models","heading":"3.3.2 Understanding Token IDs","text":"Let’s see numbers actually represent:’ll notice “inequality” becomes [500, 13237]. means:\n- word split subword tokens\n- token gets unique ID number\n- model works numbers, original text","code":"# Convert IDs back to tokens (strings)\nids = inputs.input_ids[0].tolist()\ntokens = [tokenizer.decode([i]) for i in ids]\n\nprint(\"IDs:\", ids)\nprint(\"Tokens:\", tokens)"},{"path":"llm-fundamentals.html","id":"next-token-prediction-the-core-of-llms","chapter":"3 Understanding Large Language Models","heading":"3.4 Next Token Prediction: The Core of LLMs","text":"basic level, LLMs autocomplete systems. predict next word given input text.","code":""},{"path":"llm-fundamentals.html","id":"how-it-works","chapter":"3 Understanding Large Language Models","heading":"3.4.1 How It Works","text":"Input: sequence tokens (words/subwords)Processing: model processes sequence multiple layersOutput: probability distribution possible next tokensSelection: model selects likely next token","code":""},{"path":"llm-fundamentals.html","id":"hands-on-example","chapter":"3 Understanding Large Language Models","heading":"3.4.2 Hands-On Example","text":"Let’s see action GPT-2:","code":"from transformers import GPT2TokenizerFast, GPT2LMHeadModel\nimport torch\n\n# Load the model and tokenizer\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\ndef get_next_tokens(text, num_tokens=10):\n    \"\"\"Get the most likely next tokens for a given text.\"\"\"\n    # Tokenize input\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    \n    # Get model predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits[0, -1, :]  # Last token's logits\n        probabilities = torch.softmax(logits, dim=-1)\n    \n    # Get top predictions\n    top_indices = torch.topk(probabilities, num_tokens).indices\n    top_probs = torch.topk(probabilities, num_tokens).values\n    \n    results = []\n    for idx, prob in zip(top_indices, top_probs):\n        token = tokenizer.decode([idx])\n        results.append((token, prob.item()))\n    \n    return results\n\n# Example usage\ntext = \"The future of artificial intelligence is\"\npredictions = get_next_tokens(text)\n\nprint(f\"Input: '{text}'\")\nprint(\"\\nTop 10 most likely next tokens:\")\nfor token, probability in predictions:\n    print(f\"  '{token}': {probability:.3f}\")"},{"path":"llm-fundamentals.html","id":"training-process","chapter":"3 Understanding Large Language Models","heading":"3.5 Training Process","text":"Understanding LLMs trained helps explain capabilities limitations:","code":""},{"path":"llm-fundamentals.html","id":"pre-training-phase","chapter":"3 Understanding Large Language Models","heading":"3.5.1 1. Pre-training Phase","text":"Data: Massive text corpora (books, articles, websites)Task: Predict next token sequenceDuration: Weeks months powerful hardwareCost: Millions dollars compute resources","code":""},{"path":"llm-fundamentals.html","id":"fine-tuning-phase-optional","chapter":"3 Understanding Large Language Models","heading":"3.5.2 2. Fine-tuning Phase (Optional)","text":"Data: Smaller, task-specific datasetsTask: Adapt model specific applicationsExamples: Instruction following, code generation, conversation","code":""},{"path":"llm-fundamentals.html","id":"model-sizes-and-capabilities","chapter":"3 Understanding Large Language Models","heading":"3.6 Model Sizes and Capabilities","text":"Different model sizes offer different capabilities:","code":""},{"path":"llm-fundamentals.html","id":"practical-implications","chapter":"3 Understanding Large Language Models","heading":"3.7 Practical Implications","text":"","code":""},{"path":"llm-fundamentals.html","id":"what-this-means-for-you","chapter":"3 Understanding Large Language Models","heading":"3.7.1 What This Means for You","text":"Context Windows: Models limits much text can process onceTemperature: Controls randomness generation (0 = deterministic, 1 = creative)Token Limits: Responses limited model’s maximum output lengthCost: Larger models expensive use","code":""},{"path":"llm-fundamentals.html","id":"best-practices-1","chapter":"3 Understanding Large Language Models","heading":"3.7.2 Best Practices","text":"specific: Clear prompts lead better responsesProvide context: Give model relevant background informationIterate: Refine prompts based resultsUnderstand limitations: Models can make mistakes hallucinate","code":""},{"path":"llm-fundamentals.html","id":"common-misconceptions","chapter":"3 Understanding Large Language Models","heading":"3.8 Common Misconceptions","text":"","code":""},{"path":"llm-fundamentals.html","id":"what-llms-are-not","chapter":"3 Understanding Large Language Models","heading":"3.8.1 What LLMs Are NOT","text":"databases: don’t store facts, predict based patternsNot calculators: Math abilities limited error-proneNot always factual: can generate plausible incorrect informationNot conscious: ’re sophisticated pattern matching systems","code":""},{"path":"llm-fundamentals.html","id":"what-llms-are","chapter":"3 Understanding Large Language Models","heading":"3.8.2 What LLMs ARE","text":"Pattern recognition systems: Excellent finding patterns textLanguage models: Trained understand generate human languageGeneral-purpose tools: Can adapted many different tasksStatistical systems: Based probability statistics","code":""},{"path":"llm-fundamentals.html","id":"next-steps-1","chapter":"3 Understanding Large Language Models","heading":"3.9 Next Steps","text":"Now understand LLMs work, ’re ready start using APIs. next chapter, ’ll learn interact commercial LLM services build first AI-powered applications.key takeaway LLMs powerful tools, ’re magic. Understanding limitations work make much effective user technologies.","code":""},{"path":"applications.html","id":"applications","chapter":"4 Applications","heading":"4 Applications","text":"significant applications demonstrated chapter.","code":""},{"path":"applications.html","id":"example-one","chapter":"4 Applications","heading":"4.1 Example one","text":"","code":""},{"path":"applications.html","id":"example-two","chapter":"4 Applications","heading":"4.2 Example two","text":"","code":""},{"path":"final-words.html","id":"final-words","chapter":"5 Final Words","heading":"5 Final Words","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
